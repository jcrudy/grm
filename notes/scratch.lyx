#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\use_hyperref false
\papersize default
\use_geometry false
\use_amsmath 1
\use_esint 1
\use_mhchem 1
\use_mathdots 1
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
The approximate loss function:
\begin_inset Formula 
\begin{eqnarray}
l\left(\mu_{0}+\Delta\mu,y\right) & \approx & l\left(\mu_{0},y\right)+\frac{\partial l}{\partial\mu}\left(\mu_{0},y\right)\Delta\mu+\frac{1}{2}\Delta\mu^{T}\frac{\partial^{2}l}{\partial\mu^{2}}\left(\mu_{0},y\right)\Delta\mu
\end{eqnarray}

\end_inset

The way I implement now:
\end_layout

\begin_layout Enumerate
Let 
\begin_inset Formula $\mathbf{\hat{\mu}_{0}}=\mathbf{y}$
\end_inset

 and 
\begin_inset Formula $k=0$
\end_inset


\end_layout

\begin_layout Enumerate
Until convergence, do:
\end_layout

\begin_deeper
\begin_layout Enumerate
\begin_inset Formula $\mathbf{\hat{\eta}_{k}}=g\left(\mathbf{\hat{\mu}_{k}}\right)$
\end_inset


\end_layout

\begin_layout Enumerate
\begin_inset Formula $\mathbf{z_{k}}=\mathbf{\hat{\eta}_{k}}+\left(\mathbf{y}-\mathbf{\hat{\mu}_{k}}\right)\dot{g}\left(\mathbf{\hat{\mu}_{k}}\right)$
\end_inset


\end_layout

\begin_layout Enumerate
\begin_inset Formula $\mathbf{w_{k}^{-1}}=\left(\dot{g}\left(\mathbf{\hat{\mu}_{k}}\right)\right)^{2}V\left(\mathbf{\hat{\mu}_{k}}\right)$
\end_inset


\end_layout

\begin_layout Enumerate
Regress 
\begin_inset Formula $\mathbf{z_{k}}$
\end_inset

 on 
\begin_inset Formula $\mathbf{X}$
\end_inset

 with weights 
\begin_inset Formula $\mathbf{w_{k}}$
\end_inset

 to get 
\begin_inset Formula $\mathbf{\hat{\eta}_{k+1}}$
\end_inset


\end_layout

\begin_layout Enumerate
Set 
\begin_inset Formula $k=k+1$
\end_inset


\end_layout

\end_deeper
\begin_layout Standard
Seek a loss function of the form:
\begin_inset Formula 
\begin{eqnarray}
l^{\prime}\left(\mu,y\right) & = & \left(z-\mu\right)^{T}Q\left(z-\mu\right)\\
 & = & \left(z-\mu\right)^{T}Q\left(z-\mu\right)
\end{eqnarray}

\end_inset


\begin_inset Formula 
\begin{eqnarray}
l_{i}^{\prime} & = & \sum_{i}\sum_{j}\left(z_{i}-\mu_{i}\right)Q_{ij}\left(z_{j}-\mu_{j}\right)\\
 & = & Q_{ij}\left(z_{i}-\mu_{i}\right)\left(z_{j}-\mu_{j}\right)
\end{eqnarray}

\end_inset

where 
\begin_inset Formula $g\left(\mu\right)=h\left(x\right)$
\end_inset

 that best approximates the loss function 
\begin_inset Formula $l\left(\mu\right)$
\end_inset

 near the current solution 
\begin_inset Formula $\mu^{\left(t\right)}$
\end_inset

, where 
\begin_inset Formula $Q$
\end_inset

 is symmetric positive definite.
 Well, what is the taylor expansion of 
\begin_inset Formula $l^{\prime}$
\end_inset

? Derivatives:
\begin_inset Formula 
\begin{eqnarray}
\frac{\partial l^{\prime}}{\partial\mu_{k}} & = & Q_{ij}\left(z_{j}-\mu_{j}\right)\left(\frac{\partial z_{i}}{\partial\mu_{k}}-\delta_{ik}\right)+Q_{ij}\left(z_{i}-\mu_{i}\right)\left(\frac{\partial z_{j}}{\partial\mu_{k}}-\delta_{jk}\right)\\
 & = & Q_{ij}\left(z_{j}-\mu_{j}\right)\left(\frac{\partial z_{i}}{\partial\mu_{k}}-\delta_{ik}\right)+Q_{ji}\left(z_{i}-\mu_{i}\right)\left(\frac{\partial z_{j}}{\partial\mu_{k}}-\delta_{jk}\right)\\
 & = & Q_{ij}\left(z_{j}-\mu_{j}\right)\left(\frac{\partial z_{i}}{\partial\mu_{k}}-\delta_{ik}\right)+Q_{ij}\left(z_{j}-\mu_{j}\right)\left(\frac{\partial z_{i}}{\partial\mu_{k}}-\delta_{ik}\right)\\
 & = & 2Q_{ij}\left(z_{j}-\mu_{j}\right)\left(\frac{\partial z_{i}}{\partial\mu_{k}}-\delta_{ik}\right)\\
 & = & 2Q_{ij}z_{j}\frac{\partial z_{i}}{\mu_{k}}-2Q_{ij}z_{j}\delta_{ik}\\
 &  & -2Q_{ij}\mu_{j}\frac{\partial z_{i}}{\mu_{k}}+2Q_{ij}\mu_{j}\delta_{ik}
\end{eqnarray}

\end_inset

and
\begin_inset Formula 
\begin{eqnarray}
\frac{\partial^{2}l^{\prime}}{\partial\mu_{k}\mu_{h}} & = & 2Q_{ij}\left(\frac{\partial z_{i}}{\partial\mu_{k}}-\delta_{ik}\right)\left(\frac{\partial z_{j}}{\partial\mu_{h}}-\delta_{jh}\right)+2Q_{ij}\left(z_{j}-\mu_{j}\right)\frac{\partial^{2}z_{i}}{\partial\mu_{k}\partial\mu_{h}}
\end{eqnarray}

\end_inset

Taylor expansion about 
\begin_inset Formula $\mu$
\end_inset

:
\begin_inset Formula 
\begin{eqnarray}
l^{\prime}\left(\mu+\Delta\mu\right) & \approx & Q_{ij}\left(z_{i}-\mu_{i}\right)\left(z_{j}-\mu_{j}\right)+2Q_{ij}\left(z_{j}-\mu_{j}\right)\left(\frac{\partial z_{i}}{\partial\mu_{k}}-\delta_{ik}\right)\Delta\mu_{k}\\
 &  & +Q_{ij}\left(\left(\frac{\partial z_{i}}{\partial\mu_{k}}-\delta_{ik}\right)\left(\frac{\partial z_{j}}{\partial\mu_{h}}-\delta_{jh}\right)+\left(z_{j}-\mu_{j}\right)\frac{\partial^{2}z_{i}}{\partial\mu_{k}\partial\mu_{h}}\right)\Delta\mu_{k}\Delta\mu_{h}\nonumber 
\end{eqnarray}

\end_inset

Need to solve:
\begin_inset Formula 
\begin{equation}
l^{\prime}\approx l
\end{equation}

\end_inset

for 
\begin_inset Formula $z$
\end_inset

 and 
\begin_inset Formula $Q$
\end_inset

.
 This can't be done unless we add a constant to 
\begin_inset Formula $l^{\prime}$
\end_inset

, but that's fine since we really only care about its derivatives.
 Solve the simultaneous equations:
\begin_inset Formula 
\begin{eqnarray}
2Q_{ij}\left(z_{j}-\mu_{j}\right)\left(\frac{\partial z_{i}}{\partial\mu_{k}}-\delta_{ik}\right) & = & \frac{\partial l}{\partial\mu_{k}}\\
Q_{ij}\left(\left(\frac{\partial z_{i}}{\partial\mu_{k}}-\delta_{ik}\right)\left(\frac{\partial z_{j}}{\partial\mu_{h}}-\delta_{jh}\right)+\left(z_{j}-\mu_{j}\right)\frac{\partial^{2}z_{i}}{\partial\mu_{k}\partial\mu_{h}}\right) & = & \frac{\partial^{2}l}{\partial\mu_{k}\partial\mu_{h}}
\end{eqnarray}

\end_inset

First one:
\begin_inset Formula 
\begin{eqnarray}
2Q_{ij}\left(z_{j}-\mu_{j}\right)\left(\frac{\partial z_{i}}{\partial\mu_{k}}-\delta_{ik}\right) & = & \frac{\partial l}{\partial\mu_{k}}\\
Q_{ij} & = & \frac{1}{2}\frac{\partial l}{\partial\mu_{k}}\left(\frac{\partial z_{i}}{\partial\mu_{k}}-\delta_{ik}\right)^{-1}\left(z_{j}-\mu_{j}\right)^{-1}\\
 & = & \frac{\frac{\partial l}{\partial\mu_{k}}}{2\left(\frac{\partial z_{i}}{\partial\mu_{k}}-\delta_{ik}\right)\left(z_{j}-\mu_{j}\right)}
\end{eqnarray}

\end_inset

or in matrix notation
\begin_inset Formula 
\begin{eqnarray}
2\left(\frac{\partial z}{\partial\mu}-\frac{\partial\mu}{\partial\mu}\right)Q\left(z-\mu\right) & = & \left(\frac{\partial l}{\partial\mu}\right)^{T}\\
Q\left(z-\mu\right) & = & \frac{1}{2}\left(\frac{\partial z}{\partial\mu}-\frac{\partial\mu}{\partial\mu}\right)^{-1}\left(\frac{\partial l}{\partial\mu}\right)^{T}
\end{eqnarray}

\end_inset

which is 
\begin_inset Formula 
\begin{equation}
Q_{ij}\left(z_{j}-\mu_{j}\right)=\frac{1}{2}A_{ij}\left(\frac{\partial l}{\partial\mu_{j}}\right)
\end{equation}

\end_inset

where 
\begin_inset Formula $A=\left(\frac{\partial z}{\partial\mu}-\frac{\partial\mu}{\partial\mu}\right)^{-1}$
\end_inset

.
 Second one can't be in matrix notation because it has a rank 3 tensor:
\begin_inset Formula 
\begin{eqnarray}
Q_{ij}\left(\frac{\partial z_{i}}{\partial\mu_{k}}-\delta_{ik}\right)\left(\frac{\partial z_{j}}{\partial\mu_{h}}-\delta_{jh}\right)+Q_{ij}\left(z_{j}-\mu_{j}\right)\frac{\partial^{2}z_{i}}{\partial\mu_{k}\partial\mu_{h}} & = & \frac{\partial^{2}l}{\partial\mu_{k}\partial\mu_{h}}
\end{eqnarray}

\end_inset

Assume 
\begin_inset Formula $z_{i}$
\end_inset

 depends on 
\begin_inset Formula $\mu_{k}$
\end_inset

 only if 
\begin_inset Formula $i=k$
\end_inset

.
 Then
\begin_inset Formula 
\begin{equation}
\frac{\partial l^{\prime}}{\partial\mu_{k}}=2Q_{ij}\left(z_{j}-\mu_{j}\right)\delta_{ik}\left(\frac{\partial z_{i}}{\partial\mu_{k}}-1\right)
\end{equation}

\end_inset


\begin_inset Formula 
\begin{equation}
\frac{\partial^{2}l^{\prime}}{\partial\mu_{k}\mu_{h}}=2Q_{ij}\delta_{ik}\left(\frac{\partial z_{i}}{\partial\mu_{k}}-1\right)\delta_{jh}\left(\frac{\partial z_{j}}{\partial\mu_{h}}-1\right)
\end{equation}

\end_inset

In matrix notation:
\begin_inset Formula 
\begin{equation}
\frac{\partial l^{\prime}}{\partial\mu}=Qz\frac{\partial z}{\partial\mu}-Q\mu
\end{equation}

\end_inset


\begin_inset Formula 
\begin{equation}
\frac{\partial^{2}l^{\prime}}{\partial\mu_{k}\mu_{h}}=2Q_{ij}\delta_{ik}\left(\frac{\partial z_{i}}{\partial\mu_{k}}-1\right)\delta_{jh}\left(\frac{\partial z_{j}}{\partial\mu_{h}}-1\right)
\end{equation}

\end_inset

Oh, wait you guys.
 Actually, for the purpose of the Taylor expansion I should be treating
 both 
\begin_inset Formula $z$
\end_inset

 and 
\begin_inset Formula $Q$
\end_inset

 as constant! Then
\begin_inset Formula 
\begin{equation}
\frac{\partial l^{\prime}}{\partial\mu}=-2Q\left(z-\mu\right)
\end{equation}

\end_inset

and 
\begin_inset Formula 
\begin{equation}
\frac{\partial^{2}l^{\prime}}{\partial\mu^{2}}=2Q
\end{equation}

\end_inset

and equating the Taylor terms:
\begin_inset Formula 
\begin{eqnarray}
\frac{\partial l}{\partial\mu} & = & -2Q\left(z-\mu\right)\\
\frac{\partial^{2}l}{\partial\mu^{2}} & = & 2Q
\end{eqnarray}

\end_inset

That is, 
\begin_inset Formula $Q$
\end_inset

 follows straightforwardly from the observed information matrix and 
\begin_inset Formula $z$
\end_inset

 can then be obtained by solving a linear system! I made this way too hard.
\end_layout

\begin_layout Section*
Now for real
\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $\mathbf{X}$
\end_inset

 be the data matrix with response 
\begin_inset Formula $\mathbf{y}$
\end_inset

 and let 
\begin_inset Formula $l\left(\mathbf{y},\mathbf{\eta}\right)$
\end_inset

 be the loss function.
 The loss function may be of the form 
\begin_inset Formula 
\begin{equation}
l\left(\mathbf{y},\mathbf{\eta}\right)=l_{l}\left(\mathbf{y},\mathbf{\mu}\right)
\end{equation}

\end_inset

where 
\begin_inset Formula $g\left(\mathbf{\mu}\right)=\mathbf{\eta}$
\end_inset

.
 That is, put a little 
\begin_inset Formula $l$
\end_inset

 in the subscript to denote that there's some external link function.
 Okay, so the Taylor expansion is 
\begin_inset Formula 
\begin{equation}
l\left(\eta_{0}+\mathbf{\Delta\eta}\right)\approx l\left(\mathbf{\eta_{0}}\right)+\frac{\partial l}{\partial\mathbf{\eta}}\left(\mathbf{\eta_{0}}\right)\mathbf{\Delta\eta}+\frac{1}{2}\mathbf{\Delta\eta}^{T}\frac{\partial^{2}l}{\partial\mathbf{\eta}^{2}}\left(\mathbf{\eta_{0}}\right)\mathbf{\Delta\eta}
\end{equation}

\end_inset

or with a link function 
\begin_inset Formula 
\begin{equation}
l\left(\eta_{0}+\mathbf{\Delta\eta}\right)\approx l\left(\mathbf{\eta_{0}}\right)+\frac{\partial l_{l}}{\partial\mathbf{\mu}}\left(\mathbf{\mu_{0}}\right)\frac{\partial\mathbf{\mu}}{\partial\mathbf{\eta}}\left(\mathbf{\eta}_{0}\right)\mathbf{\Delta\eta}+\frac{1}{2}\mathbf{\Delta\eta}^{T}\frac{\partial^{2}l_{l}}{\partial\mathbf{\mu}^{2}}\left(\mathbf{\mu_{0}}\right)\left(\frac{\partial\mathbf{\mu}}{\partial\mathbf{\eta}}\left(\mathbf{\eta}_{0}\right)\right)^{2}\mathbf{\Delta\eta}
\end{equation}

\end_inset

I want to approximate the loss function as a generalized least squares problem,
 such that 
\begin_inset Formula 
\begin{equation}
l\left(\mathbf{\eta}\right)\approx\mathbf{\alpha}+\left(\mathbf{z}-\mathbf{\eta}\right)^{T}\mathbf{Q}\left(\mathbf{z}-\mathbf{\eta}\right)
\end{equation}

\end_inset

for some 
\begin_inset Formula $\mathbf{\alpha}$
\end_inset

, 
\begin_inset Formula $\mathbf{z}$
\end_inset

, and 
\begin_inset Formula $\mathbf{Q}$
\end_inset

.
 I find the correct values by equating the Taylor expansions of the two
 functions.
 Firstly, let 
\begin_inset Formula $l^{\prime}\left(\mathbf{\eta}\right)=\mathbf{\alpha}+\left(\mathbf{z}-\mathbf{\eta}\right)^{T}\mathbf{Q}\left(\mathbf{z}-\mathbf{\eta}\right)$
\end_inset

.
 Since 
\begin_inset Formula $l^{\prime}$
\end_inset

 is only quadratic, it is exactly equal to its quadratic Taylor expansion,
 which is 
\begin_inset Formula 
\begin{equation}
l^{\prime}\left(\mathbf{\eta_{0}}+\mathbf{\Delta\eta}\right)=l^{\prime}\left(\mathbf{\eta_{0}}\right)+\frac{\partial l^{\prime}}{\partial\mathbf{\eta}}\left(\mathbf{\eta_{0}}\right)\mathbf{\Delta\eta}+\frac{1}{2}\mathbf{\Delta\eta}^{T}\frac{\partial^{2}l^{\prime}}{\partial\mathbf{\eta}^{2}}\left(\mathbf{\eta_{0}}\right)\mathbf{\Delta\eta}
\end{equation}

\end_inset

The derivatives are 
\begin_inset Formula 
\begin{equation}
\frac{\partial l^{\prime}}{\partial\mathbf{\eta}}\left(\mathbf{\eta_{0}}\right)=-2\mathbf{Q}\left(\mathbf{z}-\mathbf{\eta}\right)
\end{equation}

\end_inset

and 
\begin_inset Formula 
\begin{equation}
\frac{\partial^{2}l^{\prime}}{\partial\mathbf{\eta}^{2}}\left(\mathbf{\eta_{0}}\right)=2\mathbf{Q}
\end{equation}

\end_inset

and it's plain to see that 
\begin_inset Formula $l^{\prime}$
\end_inset

 can be made to exactly equal to the quadratic approximation of 
\begin_inset Formula $l$
\end_inset

 by choosing
\begin_inset Formula 
\begin{eqnarray}
\mathbf{Q} & = & \frac{1}{2}\frac{\partial^{2}l_{l}}{\partial\mathbf{\mu}^{2}}\left(\mathbf{\mu_{0}}\right)\left(\frac{\partial\mathbf{\mu}}{\partial\mathbf{\eta}}\left(\mathbf{\eta}_{0}\right)\right)^{2}\\
 & = & \frac{1}{2}\frac{\partial^{2}l_{l}}{\partial\mathbf{\mu}^{2}}\left(\mathbf{\mu_{0}}\right)\left(\frac{\partial\mathbf{\eta}}{\partial\mu}\left(\mathbf{\eta}_{0}\right)\right)^{-2}\\
\mathbf{z} & = & \mathbf{\eta}-\frac{1}{2}\mathbf{Q}^{-1}\frac{\partial l_{l}}{\partial\mathbf{\mu}}\left(\mathbf{\mu_{0}}\right)\frac{\partial\mathbf{\mu}}{\partial\mathbf{\eta}}\left(\mathbf{\eta}_{0}\right)\\
 & = & \mathbf{\eta}-\frac{1}{2}\mathbf{Q}^{-1}\frac{\partial l_{l}}{\partial\mathbf{\mu}}\left(\mathbf{\mu_{0}}\right)\left(\frac{\partial\mathbf{\eta}}{\partial\mathbf{\mu}}\right)^{-1}\left(\mathbf{\eta}_{0}\right)\\
\mathbf{\alpha} & = & l_{l}\left(\mathbf{\mu_{0}}\right)-\left(\mathbf{z}-\mathbf{\eta}\right)^{T}\mathbf{Q}\left(\mathbf{z}-\mathbf{\eta}\right)
\end{eqnarray}

\end_inset


\end_layout

\begin_layout Section*
Normal distribution
\end_layout

\begin_layout Standard
If I want to maximize the log likelihood of a normal distribution and 
\begin_inset Formula $\mathbf{\eta}=\mathbf{\mu}$
\end_inset

, then:
\begin_inset Formula 
\begin{eqnarray}
l\left(\mathbf{\mu}\right) & = & \left\Vert \left(\mathbf{\mu}-\mathbf{y}\right)\right\Vert ^{2}\\
\frac{\partial\mathbf{\mu}}{\partial\mathbf{\eta}}\left(\mathbf{\eta}_{0}\right) & = & \mathbf{I}\\
\frac{\partial l_{l}}{\partial\mathbf{\mu}} & = & 2\left(\mathbf{\mu}-\mathbf{y}\right)\\
\frac{\partial^{2}l_{l}}{\partial\mathbf{\mu}^{2}}\left(\mathbf{\mu_{0}}\right) & = & 2\mathbf{I}
\end{eqnarray}

\end_inset

so
\begin_inset Formula $\mathbf{Q}$
\end_inset

 is 
\begin_inset Formula 
\begin{equation}
\mathbf{Q}=\mathbf{I}
\end{equation}

\end_inset

Then 
\begin_inset Formula 
\begin{equation}
\mathbf{Q}^{-1}=\mathbf{I}
\end{equation}

\end_inset

and 
\begin_inset Formula 
\begin{eqnarray}
\mathbf{z} & = & \mathbf{\eta}-\frac{1}{2}\mathbf{I}2\left(\mathbf{\mu}-\mathbf{y}\right)\mathbf{I}\\
 & = & \mathbf{\eta}+\left(\mathbf{y}-\mathbf{\mu}\right)\\
 & = & \mathbf{y}
\end{eqnarray}

\end_inset

and I've recovered the regular old sum of squared errors loss function,
 like I should.
 Note that I had a choice of how exactly to write 
\begin_inset Formula $l$
\end_inset

, and I chose the way that made the elements of 
\begin_inset Formula $\mathbf{Q}$
\end_inset

 nonnegative.
\end_layout

\begin_layout Section*
Binomial distribution
\end_layout

\begin_layout Standard
The likelihood function for the binomial distribution is 
\begin_inset Formula 
\begin{equation}
L\left(\mathbf{\mu}\right)=\prod_{i}\left(\begin{array}{c}
n_{i}\\
y_{i}
\end{array}\right)\left(\mu_{i}\right)^{y_{i}}\left(1-\mu_{i}\right)^{\left(n_{i}-y_{i}\right)}
\end{equation}

\end_inset

and the negative log likelihood (neglecting the constant term) is 
\begin_inset Formula 
\begin{eqnarray}
l\left(\mathbf{\mu}\right) & = & -\sum_{i}\left[y_{i}\log\left(\mu_{i}\right)+\left(n_{i}-y_{i}\right)\log\left(1-\mu_{i}\right)\right]\\
 & = & -\sum_{i}\left[y_{i}\log\left(\frac{\mu_{i}}{1-\mu_{i}}\right)+n_{i}\log\left(1-\mu_{i}\right)\right]
\end{eqnarray}

\end_inset

Then
\begin_inset Formula 
\begin{eqnarray}
\frac{\partial l}{\partial\mathbf{\mu_{i}}} & =- & y_{i}\frac{1-\mu_{i}}{\mu_{i}}\left(-1/\mu_{i}-\frac{1-\mu_{i}}{\mu_{i}^{2}}\right)-\frac{n_{i}}{1-\mu_{i}}\\
 &  & \frac{n_{i}\mu_{i}-y_{i}}{\mu_{i}\left(1-\mu_{i}\right)}\\
 & = & \frac{n_{i}\mu_{i}-y_{i}}{\mu_{i}-\mu_{i}^{2}}\\
\frac{\partial^{2}l}{\partial\mu_{i}^{2}} & = & \frac{\mu_{i}^{2}n_{i}+y_{i}-2\mu_{i}y_{i}}{\mu_{i}^{2}\left(1-\mu_{i}\right)^{2}}
\end{eqnarray}

\end_inset

The usual thing is to take the expected value, which is
\begin_inset Formula 
\begin{eqnarray}
E\left(\frac{\partial^{2}l}{\partial\mu_{i}^{2}}\right) & = & -\frac{2\mu_{i}E\left(y_{i}\right)-\mu_{i}^{2}n_{i}-E\left(y_{i}\right)}{\left(\mu_{i}-\mu_{i}^{2}\right)^{2}}\\
 & = & -\frac{2n_{i}\mu_{i}^{2}-\mu_{i}^{2}n_{i}-n_{i}\mu_{i}}{\left(\mu_{i}-\mu_{i}^{2}\right)^{2}}\\
 & = & -\frac{n_{i}\left(\mu_{i}^{2}-\mu_{i}\right)}{\left(\mu_{i}-\mu_{i}^{2}\right)^{2}}\\
 & = & \frac{n_{i}\left(\mu_{i}-\mu_{i}^{2}\right)}{\left(\mu_{i}-\mu_{i}^{2}\right)^{2}}\\
 & = & \frac{n_{i}}{\left(\mu_{i}-\mu_{i}^{2}\right)}
\end{eqnarray}

\end_inset

which is the right answer.
 See, for example, equation (4.15) of 
\begin_inset CommandInset citation
LatexCommand cite
key "Mccullagh1989"

\end_inset

.
 Can the second derivative be negative? The highest possible value of 
\begin_inset Formula $y_{i}$
\end_inset

 is 
\begin_inset Formula $n_{i}$
\end_inset

, so look at
\begin_inset Formula 
\begin{eqnarray}
\frac{\partial^{2}l}{\partial\mu_{i}^{2}} & > & -\frac{2\mu_{i}n_{i}-\mu_{i}^{2}n_{i}-n_{i}}{\left(\mu_{i}-\mu_{i}^{2}\right)^{2}}\\
 & = & -\frac{n_{i}\left(2\mu_{i}-\mu_{i}^{2}-1\right)}{\left(\mu_{i}-\mu_{i}^{2}\right)^{2}}\\
 & = & \frac{n_{i}\left(\mu_{i}-1\right)^{2}}{\left(\mu_{i}-\mu_{i}^{2}\right)^{2}}
\end{eqnarray}

\end_inset

which is strictly postive.
 The usual link function is 
\begin_inset Formula 
\begin{equation}
\log\left(\frac{\mu_{i}}{1-\mu_{i}}\right)=\eta_{i}
\end{equation}

\end_inset

Then 
\begin_inset Formula 
\begin{eqnarray}
\mu_{i} & = & \frac{1}{1+e^{-\eta_{i}}}\\
\frac{d\mu_{i}}{d\eta_{i}} & = & \frac{e^{-\eta}}{\left(1+e^{-\eta_{i}}\right)^{2}}\\
 & = & \frac{1}{1+e^{-\eta_{i}}}\left(1-\frac{1}{1+e^{-\eta_{i}}}\right)
\end{eqnarray}

\end_inset

Okay, so using the Fisher information, 
\begin_inset Formula 
\begin{equation}
Q_{ii}=\frac{1}{2}\frac{n_{i}}{\mu_{i}\left(1-\mu_{i}\right)}\frac{e^{-2\eta}}{\left(1+e^{-\eta_{i}}\right)^{4}}
\end{equation}

\end_inset

and 
\begin_inset Formula 
\begin{eqnarray}
z_{i} & = & \eta_{i}-\frac{\mu_{i}\left(1-\mu_{i}\right)}{n_{i}}\frac{\left(1+e^{-\eta_{i}}\right)^{4}}{e^{-2\eta}}\left(\frac{n_{i}\mu_{i}-y_{i}}{\mu_{i}\left(1-\mu_{i}\right)}\right)\left(\frac{e^{-\eta}}{\left(1+e^{-\eta_{i}}\right)^{2}}\right)\\
 & = & \eta_{i}-\left(\mu_{i}-\frac{y_{i}}{n_{i}}\right)\frac{\left(1+e^{-\eta_{i}}\right)^{2}}{e^{-\eta}}
\end{eqnarray}

\end_inset

or for general link functions,
\begin_inset Formula 
\begin{eqnarray}
Q_{ii} & = & \frac{1}{2}\frac{n_{i}}{\mu_{i}\left(1-\mu_{i}\right)}\left(\frac{\partial\eta_{i}}{\partial\mu_{i}}\right)^{-2}\\
z_{i} & = & \eta_{i}+\left(\frac{y_{i}}{n_{i}}-\mu_{i}\right)\left(\frac{\partial\eta_{i}}{\partial\mu_{i}}\right)
\end{eqnarray}

\end_inset

Or, using the observed information,
\begin_inset Formula 
\begin{equation}
Q_{ii}=\frac{1}{2}\frac{\mu_{i}^{2}n_{i}+y_{i}-2\mu_{i}y_{i}}{\mu_{i}^{2}\left(1-\mu_{i}\right)^{2}}\frac{e^{-2\eta}}{\left(1+e^{-\eta_{i}}\right)^{4}}
\end{equation}

\end_inset

and 
\begin_inset Formula 
\begin{eqnarray}
z_{i} & = & \eta_{i}-\left(\frac{\mu_{i}^{2}\left(1-\mu_{i}\right)^{2}}{\mu_{i}^{2}n_{i}+y_{i}-2\mu_{i}y_{i}}\right)\left(\frac{\left(1+e^{-\eta_{i}}\right)^{4}}{e^{-2\eta}}\right)\left(\frac{n_{i}\mu_{i}-y_{i}}{\mu_{i}\left(1-\mu_{i}\right)}\right)\left(\frac{e^{-\eta}}{\left(1+e^{-\eta_{i}}\right)^{2}}\right)\nonumber \\
 & = & \eta_{i}-\left(\frac{n_{i}\mu_{i}\left(1-\mu_{i}\right)}{\mu_{i}^{2}n_{i}+y_{i}-2\mu_{i}y_{i}}\right)\left(\mu_{i}-\frac{y_{i}}{n_{i}}\right)\left(\frac{\left(1+e^{-\eta_{i}}\right)^{2}}{e^{-\eta}}\right)
\end{eqnarray}

\end_inset

or with general link function,
\begin_inset Formula 
\begin{eqnarray}
Q_{ii} & = & \frac{1}{2}\frac{\mu_{i}^{2}n_{i}+y_{i}-2\mu_{i}y_{i}}{\mu_{i}^{2}\left(1-\mu_{i}\right)^{2}}\left(\frac{\partial\eta_{i}}{\partial\mu_{i}}\right)^{-2}\\
z_{i} & = & \eta_{i}+\left(\frac{n_{i}\mu_{i}\left(1-\mu_{i}\right)}{\mu_{i}^{2}n_{i}+y_{i}-2\mu_{i}y_{i}}\right)\left(\frac{y_{i}}{n_{i}}-\mu_{i}\right)\frac{\partial\eta_{i}}{\partial\mu_{i}}
\end{eqnarray}

\end_inset


\end_layout

\begin_layout Section*
Hazard function
\end_layout

\begin_layout Standard
Let 
\begin_inset Formula 
\begin{eqnarray}
h\left(\mathbf{x}_{i},t\right) & = & e^{\phi\left(\mathbf{x}_{i},t\right)}
\end{eqnarray}

\end_inset

be the hazard function for some counting process, where 
\begin_inset Formula $\phi$
\end_inset

 is the function represented by the inner regressor.
 Let 
\begin_inset Formula $\mathbf{y}$
\end_inset

 be the times and 
\begin_inset Formula $\mathbf{c}$
\end_inset

 be the censorship status, where 
\begin_inset Formula $\mathbf{c}_{i}=0$
\end_inset

 indicates censorship.
 The likelihood function is
\begin_inset Formula 
\begin{equation}
L=\prod_{i=1}^{n}f\left(\mathbf{x},y_{i}\right)^{c_{i}}\left(S\left(\mathbf{x},y_{i}\right)\right)^{1-c_{i}}
\end{equation}

\end_inset

where
\begin_inset Formula 
\begin{equation}
f\left(\mathbf{x},t\right)=S\left(\mathbf{x},t\right)h\left(\mathbf{x},t\right)
\end{equation}

\end_inset

and
\begin_inset Formula 
\begin{equation}
F\left(\mathbf{x},t\right)=1-S\left(\mathbf{x},t\right)
\end{equation}

\end_inset

and 
\begin_inset Formula 
\begin{equation}
S\left(\mathbf{x},t\right)=e^{-H\left(\mathbf{x},t\right)}
\end{equation}

\end_inset

and 
\begin_inset Formula 
\begin{equation}
H\left(\mathbf{x},t\right)=\int_{0}^{t}h\left(\mathbf{x},\tau\right)d\tau
\end{equation}

\end_inset

The log likelihood is 
\begin_inset Formula 
\begin{equation}
l=\sum_{i=1}^{n}c_{i}\phi\left(\mathbf{x},y_{i}\right)-\int_{0}^{y_{i}}e^{\phi\left(\mathbf{x},\tau\right)}d\tau
\end{equation}

\end_inset

Let's approximate the integral using some rule such that
\begin_inset Formula 
\begin{eqnarray}
\int_{0}^{t}e^{\phi\left(\mathbf{x},\tau\right)}d\tau & = & \sum_{i=1}^{p}b_{i}\\
b_{i} & = & g\left(t_{\left(i-1\right)},t_{\left(i\right)},e^{\phi\left(\mathbf{x}_{\left(i\right)},t_{\left(i\right)}\right)}\right)
\end{eqnarray}

\end_inset

for some 
\begin_inset Formula $g$
\end_inset

, where 
\begin_inset Formula $p$
\end_inset

 is the total number of sample points with time less than 
\begin_inset Formula $t$
\end_inset

.
 Then
\begin_inset Formula 
\begin{equation}
l=\sum_{i=1}^{n}c_{i}\mu_{i}-\sum_{i=1}^{n-1}b_{i}
\end{equation}

\end_inset

The derivatives are 
\begin_inset Formula 
\begin{eqnarray}
\frac{\partial l}{\partial\mu_{i}} & = & c_{i}-\frac{\partial b_{i}}{\partial\mu_{i}}\\
\frac{\partial^{2}l}{\partial\mu_{i}\partial\mu_{j}} & = & -\frac{\partial^{2}b_{i}}{\partial\mu_{i}\partial\mu_{j}}
\end{eqnarray}

\end_inset

If 
\begin_inset Formula $g$
\end_inset

 is the usual rectangle rule, then 
\begin_inset Formula 
\begin{equation}
g\left(t_{\left(i-1\right)},t_{\left(i\right)},e^{\phi\left(\mathbf{x}_{\left(i\right)},t_{\left(i\right)}\right)}\right)=\left(t_{\left(i\right)}-t_{\left(i-1\right)}\right)e^{\phi\left(\mathbf{x}_{\left(i\right)},t_{\left(i\right)}\right)}
\end{equation}

\end_inset

So the new approximate loss function is 
\begin_inset Formula 
\begin{equation}
l=\left(\sum_{i=1}^{n}c_{i}\phi\left(\mathbf{x}_{i},y_{i}\right)\right)-\sum_{i=1}^{p}\left(t_{\left(i\right)}-t_{\left(i-1\right)}\right)e^{\phi\left(\mathbf{x}_{\left(i\right)},t_{\left(i\right)}\right)}
\end{equation}

\end_inset

and the observed information matrix will be diagonal! Is it fair to treat
 the integral this way? Intuitively, it means that areas of higher hazard
 will have higher precision in the integral approximation, which seems reasonabl
e.
 It also means that areas of higher censorship will have higher resolution,
 which is less good but okay I guess.
 Note that I used a rectangle rule instead of a trapezoid rule in order
 to avoid having to deal with 
\begin_inset Formula $t=0$
\end_inset

.
\end_layout

\begin_layout Section*
Hazard function more clearly, concisely, and correctly
\end_layout

\begin_layout Standard
The goal is to fit a log hazard function
\begin_inset Formula 
\begin{equation}
\log\left(h\left(\mathbf{X},\mathbf{t}\right)\right)=\mathbf{\eta}
\end{equation}

\end_inset

where 
\begin_inset Formula $\mathbf{\eta}=\phi\left(\mathbf{X},\mathbf{t}\right)$
\end_inset

 is the prediction of the inner regressor and 
\begin_inset Formula $\mathbf{t}$
\end_inset

 is time since baseline.
 The above model leads to the following loss function
\begin_inset Formula 
\begin{equation}
l=-\sum_{i=1}^{n}\left(c_{i}\phi\left(\mathbf{x}_{i},y_{i}\right)-\int_{0}^{y_{i}}e^{\phi\left(\mathbf{x}_{i},\tau\right)}d\tau\right)
\end{equation}

\end_inset

which is the negative of the log likelihood function.
 Here 
\begin_inset Formula $\mathbf{c}$
\end_inset

 is the censorship type (
\begin_inset Formula $c_{i}=1$
\end_inset

 indicates event/failure/death) and 
\begin_inset Formula $\mathbf{y}$
\end_inset

 is the time.
 Let 
\begin_inset Formula $y_{\left(i\right)}$
\end_inset

 be the 
\begin_inset Formula $i^{th}$
\end_inset

 order statistic of 
\begin_inset Formula $\mathbf{y}$
\end_inset

 and 
\begin_inset Formula $\mathbf{x}_{i}$
\end_inset

 the associated predictors and so on.
 Using the rectangle method, approximate the integral in the loss function
 by 
\begin_inset Formula 
\begin{eqnarray}
\int_{0}^{y_{i}}e^{\phi\left(\mathbf{x}_{i},\tau\right)}d\tau & = & \sum_{j=1}^{n}I_{\left(j\right)\le\left\langle i\right\rangle }\left(y_{\left(j\right)}-y_{\left(j-1\right)}\right)e^{\phi\left(\mathbf{x}_{\left(j\right)},y_{\left(j\right)}\right)}\\
 & = & \sum_{j=1}^{n}I_{\left(j\right)\le\left\langle i\right\rangle }\left(y_{\left(j\right)}-y_{\left(j-1\right)}\right)e^{\eta_{\left(j\right)}}\\
 & = & \sum_{j=1}^{n}I_{y_{\left(j\right)}\le y_{i}}\left(y_{\left(j\right)}-y_{\left(j-1\right)}\right)e^{\eta_{\left(j\right)}}
\end{eqnarray}

\end_inset

where I've used 
\begin_inset Formula $\left\langle i\right\rangle $
\end_inset

 to indicate the rank of 
\begin_inset Formula $i$
\end_inset

.
 (Basically, 
\begin_inset Formula $\left\langle \right\rangle $
\end_inset

 is the inverse of 
\begin_inset Formula $\left(\right)$
\end_inset

, so that 
\begin_inset Formula $i=\left\langle \left(i\right)\right\rangle =\left(\left\langle i\right\rangle \right)$
\end_inset

.
 I hope that makes sense.
 I'm just switching between the original order and rank-based order.) The
 approximated loss function becomes
\begin_inset Formula 
\begin{eqnarray}
l & = & -\sum_{i=1}^{n}\left(c_{i}\eta_{i}-\sum_{j=1}^{n}I_{y_{\left(j\right)}\le y_{i}}\left(y_{\left(j\right)}-y_{\left(j-1\right)}\right)e^{\eta_{\left(j\right)}}\right)\\
 & = & -\sum_{i=1}^{n}\left(c_{i}\mu_{i}-\sum_{j=1}^{n}I_{y_{\left(j\right)}\le y_{i}}\left(y_{\left(j\right)}-y_{\left(j-1\right)}\right)e^{\mu_{\left(j\right)}}\right)
\end{eqnarray}

\end_inset

Let 
\begin_inset Formula $\upsilon_{i}=y_{i}-y_{\left(\left\langle i\right\rangle -1\right)}$
\end_inset

.
 Then 
\begin_inset Formula 
\begin{eqnarray}
l & = & -\sum_{i=1}^{n}\left(c_{i}\mu_{i}-\sum_{j=1}^{n}I_{y_{j}\le y_{i}}\nu_{j}e^{\mu_{j}}\right)\\
 & = & -\sum_{i=1}^{n}c_{i}\mu_{i}+\sum_{i=1}^{n}\sum_{j=1}^{n}I_{y_{j}\le y_{i}}\nu_{j}e^{\mu_{j}}\\
 & = & -\sum_{i=1}^{n}c_{i}\mu_{i}+\sum_{j=1}^{n}\nu_{j}e^{\mu_{j}}\sum_{i=1}^{n}I_{y_{j}\le y_{i}}\\
 & = & -\sum_{i=1}^{n}c_{i}\mu_{i}+\sum_{j=1}^{n}\nu_{j}e^{\mu_{j}}N_{j}
\end{eqnarray}

\end_inset

The score function is 
\begin_inset Formula 
\begin{eqnarray}
\frac{\partial l}{\partial\mu_{k}} & = & -\sum_{i=1}^{n}\left(c_{i}\delta_{ik}-\sum_{j=1}^{n}I_{y_{\left(j\right)}\le y_{i}}\left(y_{\left(j\right)}-y_{\left(j-1\right)}\right)e^{\mu_{\left(j\right)}}\delta_{\left(j\right)k}\right)\\
 & = & -c_{k}+\sum_{i=1}^{n}\sum_{j=1}^{n}\delta_{\left(j\right)k}I_{y_{\left(j\right)}\le y_{i}}\left(y_{\left(j\right)}-y_{\left(j-1\right)}\right)e^{\mu_{\left(j\right)}}\\
 & = & -c_{k}+\sum_{i=1}^{n}I_{y_{k}\le y_{i}}\left(y_{k}-y_{\left(\left\langle k\right\rangle -1\right)}\right)e^{\mu_{k}}\\
 & = & -c_{k}+\sum_{i=1}^{n}I_{y_{k}\le y_{i}}\nu_{k}e^{\mu_{k}}\\
 & = & -c_{k}+\nu_{k}e^{\mu_{k}}\sum_{i=1}^{n}I_{y_{k}\le y_{i}}
\end{eqnarray}

\end_inset

and the observed information matrix is diagonal and is given by 
\begin_inset Formula 
\begin{eqnarray}
\frac{\partial^{2}l}{\partial\mu_{k}\partial\mu_{k}} & = & \sum_{i=1}^{n}I_{y_{k}\le y_{i}}\nu_{k}e^{\mu_{k}}\\
 & = & \nu_{k}e^{\mu_{k}}\sum_{i=1}^{n}I_{y_{k}\le y_{i}}
\end{eqnarray}

\end_inset

Obviously the approximation of the integral is very convenient.
 Note that 
\begin_inset Formula $\sum_{i=1}^{n}I_{y_{k}\le y_{i}}$
\end_inset

 is just the number of 
\begin_inset Formula $y_{i}$
\end_inset

 in the sample that are greater than 
\begin_inset Formula $y_{k}$
\end_inset

.
 Let's call it 
\begin_inset Formula 
\begin{equation}
N_{k}=\sum_{i=1}^{n}I_{y_{k}\le y_{i}}
\end{equation}

\end_inset

So that 
\begin_inset Formula 
\begin{eqnarray}
\frac{\partial l}{\partial\mu_{k}} & = & \nu_{k}e^{\mu_{k}}N_{k}-c_{k}\\
\frac{\partial^{2}l}{\partial\mu_{k}\partial\mu_{k}} & = & \nu_{k}e^{\mu_{k}}N_{k}
\end{eqnarray}

\end_inset

Note that 
\begin_inset Formula $\nu_{k}>0$
\end_inset

 and 
\begin_inset Formula $N_{k}>0$
\end_inset

 for all 
\begin_inset Formula $k$
\end_inset

, so the loss function is appropriately convex (or concave, if you like).
 Then 
\begin_inset Formula 
\begin{eqnarray}
Q_{kk} & = & \frac{1}{2}\nu_{k}e^{\mu_{k}}N_{k}\\
z_{k} & = & \mathbf{\eta}_{k}-\frac{1}{\nu_{k}N_{k}}e^{-\mu_{k}}\left(\nu_{k}e^{\mu_{k}}N_{k}-c_{k}\right)\\
 & = & \eta_{k}+\frac{c_{k}}{\nu_{k}N_{k}}e^{-\mu_{k}}-1
\end{eqnarray}

\end_inset

For a starting point, the unconstrained solution is 
\begin_inset Formula 
\begin{eqnarray}
0 & = & \frac{\partial l}{\partial\mu_{k}}\\
 & = & \nu_{k}e^{\mu_{k}}N_{k}-c_{k}\\
c_{i} & = & \nu_{k}e^{\mu_{k}}N_{k}\\
\log\left(\frac{c_{k}}{\nu_{k}N_{k}}\right) & = & \mu_{k}
\end{eqnarray}

\end_inset


\end_layout

\begin_layout Section*
Other approximations of the integral (the last one was wrong)
\end_layout

\begin_layout Standard
I chose the rectangle rule that I did above because it allowed me to avoid
 computing the predicted log hazard at any new points.
 However, what I did isn't entirely legitimate.
 Specifically, I ignored the fact that 
\begin_inset Formula $\mathbf{x}_{i}\ne\mathbf{x}_{j}$
\end_inset

 for general 
\begin_inset Formula $i$
\end_inset

 and 
\begin_inset Formula $j$
\end_inset

.
 I think that's pretty unforgiveable.
 I need a better way to approximate the integral, and I think the only solution
 must be to create additional 
\begin_inset Quotes eld
\end_inset

phantom
\begin_inset Quotes erd
\end_inset

 sample points representing points at which the inner regressor is evaluated
 to approximate the integral.
 Obviously there is a drawback to this method: what was a 1000 sample problem
 could easily become a 100000 or 1000000 sample problem, which takes way
 more time and memory.
 However, let's pursue this avenue for now and see where it leads.
\end_layout

\begin_layout Standard
Instead of what I said before, let's approximate the integral as 
\begin_inset Formula 
\begin{equation}
\int_{0}^{y_{i}}e^{\phi\left(\mathbf{x}_{i},\tau\right)}d\tau=\sum_{\iota=0}^{\omega}b\left(\iota,\omega,y_{i}\right)e^{\phi\left(\mathbf{x},\iota y_{i}/\omega\right)}
\end{equation}

\end_inset

for some 
\begin_inset Formula $b$
\end_inset

, where 
\begin_inset Formula $\omega$
\end_inset

 is the number of points we're using to approximate the integral.
 This formulation covers the rectangle, trapezoid, and Simpson's rules.
 Probably others as well.
 The loss function was
\begin_inset Formula 
\begin{equation}
l=-\sum_{i=1}^{n}\left(c_{i}\phi\left(\mathbf{x}_{i},y_{i}\right)-\int_{0}^{y_{i}}e^{\phi\left(\mathbf{x}_{i},\tau\right)}d\tau\right)
\end{equation}

\end_inset

so with this approximation it's 
\begin_inset Formula 
\begin{eqnarray}
l & \approx & -\sum_{i=1}^{n}\left(c_{i}\phi\left(\mathbf{x}_{i},y_{i}\right)-\sum_{\iota=0}^{\omega_{i}}b\left(\iota,\omega_{i},y_{i}\right)e^{\phi\left(\mathbf{x}_{i},\iota y_{i}/\omega_{i}\right)}\right)\\
 & = & -\sum_{i=1}^{n}c_{i}\phi\left(\mathbf{x}_{i},y_{i}\right)+\sum_{i=1}^{n}\sum_{\iota=0}^{\omega_{i}}b\left(\iota,\omega_{i},y_{i}\right)e^{\phi\left(\mathbf{x}_{i},\iota y_{i}/\omega_{i}\right)}\\
 & = & -\sum_{i=1}^{n}c_{i}\mu_{\Upsilon_{i\omega}}+\sum_{i=1}^{n}\sum_{\iota=0}^{\omega_{i}}b\left(\iota,\omega_{i},y_{i}\right)e^{\mu_{\Upsilon_{i\iota}}}
\end{eqnarray}

\end_inset

Note that there are now 
\begin_inset Formula $n\left(\omega+1\right)$
\end_inset

 points at which the inner regressor must be fit and evaluated.
 Okay, that's fine I guess.
 It still fits within the framework.
 We still have 
\begin_inset Formula $\mu_{i}=\eta_{i}=\phi\left(\mathbf{x}_{i},y_{i}\right)$
\end_inset

.
 However, now there is also 
\begin_inset Formula $\mu_{n+\Upsilon_{i\iota}}=\phi\left(\mathbf{x}_{i},\iota y_{i}/\omega\right)$
\end_inset

 where I've defined 
\begin_inset Formula $\Upsilon_{i\iota}$
\end_inset

 as the convenient indexing function that will keep everything organized
 somehow.
 But remember that 
\begin_inset Formula $\mu_{n+\Upsilon_{i\omega}}=\mu_{i}$
\end_inset

, so let's drop the first 
\begin_inset Formula $n$
\end_inset

 and just use the 
\begin_inset Formula $\mu_{\Upsilon_{i\iota}}$
\end_inset

.
 This probably doesn't make sense to anyone else, but it will keep me sane
 for the moment.
 The score function is 
\begin_inset Formula 
\begin{eqnarray}
\frac{\partial l}{\partial\mu_{\Upsilon_{kh}}} & \approx & -c_{k}\delta_{h\omega}+\sum_{i=1}^{n}\delta_{ki}\sum_{\iota=0}^{\omega_{i}}\delta_{h\iota}b\left(\iota,\omega_{i},y_{i}\right)e^{\mu_{\Upsilon_{i\iota}}}\\
 & = & -c_{k}\delta_{h\omega}+b\left(h,\omega_{k},y_{k}\right)e^{\mu_{\Upsilon_{kh}}}
\end{eqnarray}

\end_inset

and the diagonal observed information is 
\begin_inset Formula 
\begin{equation}
\frac{\partial^{2}l}{\partial\mu_{\Upsilon_{kh}}^{2}}\approx b\left(h,\omega_{k},y_{k}\right)e^{\mu_{\Upsilon_{kh}}}
\end{equation}

\end_inset

At least this math is quite simple.
 To review: 
\begin_inset Formula 
\begin{eqnarray}
l & \approx & -\sum_{i=1}^{n}c_{i}\mu_{\Upsilon_{i\omega}}+\sum_{i=1}^{n}\sum_{\iota=0}^{\omega_{i}}b\left(\iota,\omega_{i},y_{i}\right)e^{\mu_{\Upsilon_{i\iota}}}\\
\frac{\partial l}{\partial\mu_{\Upsilon_{kh}}} & \approx & -c_{k}\delta_{h\omega_{k}}+b\left(h,\omega_{k},y_{k}\right)e^{\mu_{\Upsilon_{kh}}}\\
\frac{\partial^{2}l}{\partial\mu_{\Upsilon_{kh}}^{2}} & \approx & b\left(h,\omega_{k},y_{k}\right)e^{\mu_{\Upsilon_{kh}}}
\end{eqnarray}

\end_inset

This approximation is not as convenient as my previous one.
 It is, however, infinitely more correct.
 The new 
\begin_inset Formula $\mathbf{z}$
\end_inset

 and 
\begin_inset Formula $\mathbf{Q}$
\end_inset

 are 
\begin_inset Formula 
\begin{eqnarray}
Q_{\Upsilon_{kh}\Upsilon_{kh}} & = & \frac{1}{2}b\left(h,\omega_{k},y_{k}\right)e^{\mu_{\Upsilon_{kh}}}\\
z_{\Upsilon_{kh}} & = & \mathbf{\eta_{\Upsilon_{kh}}}-\frac{1}{b\left(h,\omega_{k},y_{k}\right)}e^{-\mu_{\Upsilon_{kh}}}\left(-c_{k}\delta_{h\omega}+b\left(h,\omega_{k},y_{k}\right)e^{\mu_{\Upsilon_{kh}}}\right)\\
 & = & \mathbf{\eta_{\Upsilon_{kh}}}-\left(-\frac{c_{k}\delta_{h\omega_{k}}}{b\left(h,\omega_{k},y_{k}\right)}e^{-\mu_{\Upsilon_{kh}}}+1\right)\\
 & = & \mu_{\Upsilon_{kh}}+\left(\frac{c_{k}\delta_{h\omega_{k}}}{b\left(h,\omega_{k},y_{k}\right)}e^{-\mu_{\Upsilon_{kh}}}-1\right)\\
 & = & \mu_{\Upsilon_{kh}}+\frac{1}{b\left(h,\omega_{k},y_{k}\right)}\left(c_{k}\delta_{h\omega_{k}}e^{-\mu_{\Upsilon_{kh}}}-b\left(h,\omega_{k},y_{k}\right)\right)
\end{eqnarray}

\end_inset

The unconstrained solution is 
\begin_inset Formula 
\begin{eqnarray}
0 & = & \frac{\partial l}{\partial\mu_{\Upsilon_{kh}}}\\
 & = & -c_{k}\delta_{h\omega_{k}}+b\left(h,\omega_{k},y_{k}\right)e^{\mu_{\Upsilon_{kh}}}\\
c_{k}\delta_{h\omega_{k}} & = & b\left(h,\omega_{k},y_{k}\right)e^{\mu_{\Upsilon_{kh}}}\\
e^{\mu_{\Upsilon_{kh}}} & = & \frac{c_{k}\delta_{h\omega_{k}}}{b\left(h,\omega_{k},y_{k}\right)}\\
\mu_{\Upsilon_{kh}} & = & \log\left(\frac{c_{k}\delta_{h\omega_{k}}}{b\left(h,\omega_{k},y_{k}\right)}\right)
\end{eqnarray}

\end_inset

but of course this solution gives an infinitely negative loss with the hazard
 being zero everywhere except the event points.
 A good starting point might be some shrunken version, 
\begin_inset Formula 
\begin{equation}
\tilde{\mu}_{\Upsilon_{kh}}=\log\left(\frac{\left(\left(1-2\epsilon\right)\delta_{h\omega_{k}}c_{k}+\epsilon\right)}{b\left(h,\omega_{k},y_{k}\right)}\right)
\end{equation}

\end_inset

where 
\begin_inset Formula $\epsilon$
\end_inset

 is some fairly small number, like 
\begin_inset Formula $\frac{1}{10}$
\end_inset

 or something.
\end_layout

\begin_layout Subsection*
Rectangle rule
\end_layout

\begin_layout Standard
If we're using the rectangle rule then 
\begin_inset Formula 
\begin{equation}
b\left(\iota,\omega,y\right)=\frac{y}{\omega}\left(1-\delta_{\iota0}\right)
\end{equation}

\end_inset

for the top right corner approximation.
 For the top left corner approximation, it's 
\begin_inset Formula 
\begin{equation}
b\left(\iota,\omega,y\right)=\frac{y}{\omega}\left(1-\delta_{\iota\omega}\right)
\end{equation}

\end_inset

and for the midpoint approximation it's
\begin_inset Formula 
\begin{equation}
b\left(\iota,\omega,y\right)=\frac{y}{\omega}\left(1-\frac{1}{2}\left(\delta_{\iota0}+\delta_{\iota\omega}\right)\right)
\end{equation}

\end_inset


\end_layout

\begin_layout Subsection*
Trapezoid rule
\end_layout

\begin_layout Standard
Because the selection of sample points isn't changing, the trapezoid rule
 is identical to the midpoint rectangle rule above.
\end_layout

\begin_layout Subsection*
Others
\end_layout

\begin_layout Standard
It might be good to look at Simpson's rule and Gaussian quadrature at some
 point.
\end_layout

\begin_layout Section*
Penalized complexity
\end_layout

\begin_layout Standard
MARS doesn't actually minimize squared error.
 I guess I lied a little.
 Instead, it minimizes GCV, which is a squared error that is also penalized
 by the complexity of the model.
 The GCV is
\begin_inset Formula 
\begin{equation}
GCV\left(\mathbf{y},\mathbf{\eta}\right)=\alpha+\beta\left(\mathbf{z}-\mathbf{\eta}\right)^{T}\mathbf{Q}\left(\mathbf{z}-\mathbf{\eta}\right)
\end{equation}

\end_inset

where 
\begin_inset Formula 
\begin{equation}
\beta=\frac{1}{N}\left(1-\frac{C\left(M\right)}{N}\right)^{-2}
\end{equation}

\end_inset

Fortunately, 
\begin_inset Formula $\beta$
\end_inset

 does not depend on 
\begin_inset Formula $\mathbf{\eta}$
\end_inset

.
 How should this adjustment affect 
\begin_inset Formula $\mathbf{z}$
\end_inset

 and 
\begin_inset Formula $\mathbf{W}$
\end_inset

? More importantly, how should sample weighting be accounted for in 
\begin_inset Formula $\beta$
\end_inset

? I propose the following:
\begin_inset Formula 
\begin{eqnarray}
\beta & = & \frac{1}{tr\left(\mathbf{Q}\right)}\left(1-\frac{C\left(M\right)}{tr\left(\mathbf{Q}\right)}\right)^{-2}\\
 & = & tr\left(\mathbf{Q}\right)^{-1}\left(1-\frac{C\left(M\right)}{tr\left(\mathbf{Q}\right)}\right)^{-2}\\
 & = & \sqrt{tr\left(\mathbf{Q}\right)}^{-2}\left(1-\frac{C\left(M\right)}{tr\left(\mathbf{Q}\right)}\right)^{-2}\\
 & = & \left(\sqrt{tr\left(\mathbf{Q}\right)}-\frac{C\left(M\right)}{\sqrt{tr\left(\mathbf{Q}\right)}}\right)^{-2}
\end{eqnarray}

\end_inset

for diagonal 
\begin_inset Formula $\mathbf{Q}$
\end_inset

, which is the only kind I am currently considering.
 A few observations:
\end_layout

\begin_layout Enumerate
During pruning, for a fixed 
\begin_inset Formula $N$
\end_inset

 and fixed 
\begin_inset Formula $\mathbf{Q}$
\end_inset

, this change in GCV will not affect the order in which terms are pruned.
\end_layout

\begin_layout Enumerate
This change in GCV will affect which step in pruning is the optimal step.
 That is, it will affect the final number of terms pruned.
\end_layout

\begin_layout Enumerate
This change in GCV will affect whether linear terms are chosed in allow_linear=T
rue.
\end_layout

\begin_layout Standard
The Taylor series for the GCV is different than for squared error loss and
 will produce different solutions.
 Let 
\begin_inset Formula $\mathbf{\eta}=\mathbf{\eta_{0}}+\mathbf{\Delta\eta}$
\end_inset

 and 
\begin_inset Formula $l^{\prime}\left(\mathbf{\eta}\right)=GCV\left(\mathbf{y},\mathbf{\eta}\right)$
\end_inset

.
 The Taylor series is 
\begin_inset Formula 
\begin{equation}
l^{\prime}\left(\mathbf{\eta_{0}}+\mathbf{\Delta\eta}\right)=l^{\prime}\left(\mathbf{\eta_{0}}\right)+\frac{\partial l^{\prime}}{\partial\mathbf{\eta}}\left(\mathbf{\eta_{0}}\right)\mathbf{\Delta\eta}+\frac{1}{2}\mathbf{\Delta\eta}^{T}\frac{\partial^{2}l^{\prime}}{\partial\mathbf{\eta}^{2}}\left(\mathbf{\eta_{0}}\right)\mathbf{\Delta\eta}
\end{equation}

\end_inset

and 
\begin_inset Formula 
\begin{eqnarray}
\frac{\partial l^{\prime}}{\partial\mathbf{\eta}} & = & -2\beta\mathbf{Q}\left(\mathbf{z}-\mathbf{\eta}\right)\\
\frac{\partial^{2}l^{\prime}}{\partial\mathbf{\eta}^{2}} & = & 2\beta\mathbf{Q}
\end{eqnarray}

\end_inset

Perhaps a better solution would be to disable pruning altogether and instead
 use a penalized loss function.
 If I stick with my original formulation, the resulting ILFA loss function
 is a Taylor approximation for ...
\end_layout

\begin_layout Section*
Getting into MARS
\end_layout

\begin_layout Standard
What if instead of GCV I used a roughness penalty in MARS?
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "notes"
options "plain"

\end_inset


\end_layout

\end_body
\end_document
