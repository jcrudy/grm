#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\use_hyperref false
\papersize default
\use_geometry false
\use_amsmath 1
\use_esint 1
\use_mhchem 1
\use_mathdots 1
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
I've been reading 
\emph on
Generalized Linear Models 
\begin_inset CommandInset citation
LatexCommand cite
key "Mccullagh1989"

\end_inset

,
\emph default
 which presents generalized linear models as an extension of general linear
 models in a very clear and thoughtful way.
 My favorite part so far is section 2.2.1, entitled 
\begin_inset Quotes eld
\end_inset

The Generalization,
\begin_inset Quotes erd
\end_inset

 reproduced below:
\end_layout

\begin_layout Quote
2.2.1 The generalization
\end_layout

\begin_layout Quote
To simplify the transition to generalized inear models, we shall rearrange
 
\emph on
[the definition of general linear models]
\emph default
 slightly to produce the following three part specification:
\end_layout

\begin_layout Quote
1.
 The 
\emph on
random component:
\emph default
 the components of 
\begin_inset Formula $\mathbf{Y}$
\end_inset

 have independent Normal distributions with 
\begin_inset Formula $E\left(\mathbf{Y}\right)=\mathbf{\mu}$
\end_inset

 and constant variance 
\begin_inset Formula $\sigma^{2}$
\end_inset

;
\end_layout

\begin_layout Quote
2.
 The 
\emph on
systematic component:
\emph default
 covariates 
\begin_inset Formula $\mathbf{x_{1}},\mathbf{x_{2}},\ldots,\mathbf{x_{p}}$
\end_inset

 produce a 
\emph on
linear predictor
\emph default
 
\begin_inset Formula $\mathbf{\eta}$
\end_inset

 given by 
\begin_inset Formula 
\begin{equation}
\mathbf{\eta}=\sum_{1}^{p}\mathbf{x_{j}}\beta_{j};
\end{equation}

\end_inset


\end_layout

\begin_layout Quote
3.The 
\emph on
link
\emph default
 between the random and systematic components:
\begin_inset Formula 
\begin{equation}
\mathbf{\mu}=\mathbf{\eta}
\end{equation}

\end_inset


\end_layout

\begin_layout Quote
This generalization introduces a new symbol 
\begin_inset Formula $\mathbf{\eta}$
\end_inset

 for the linear predictor and the third component then specifes that 
\begin_inset Formula $\mu$
\end_inset

 and 
\begin_inset Formula $\eta$
\end_inset

 are in fact identical.
 If we write
\begin_inset Formula 
\begin{equation}
\mathbf{\eta_{i}}=g\left(\mathbf{\mu_{i}}\right),
\end{equation}

\end_inset

then 
\begin_inset Formula $g\left(\cdot\right)$
\end_inset

 will be called the 
\emph on
link function.

\emph default
 In this formulation, classical linear models have a Normal (or Gaussian)
 distribution in component 1 and the identity function for the link in component
 3.
 Generalized linear models allow two extensions; first the distribution
 in component 1 may come from an exponential family other than the Normal,
 and secondly the link function in component 3 may become any monotonic
 differentiable function.
\end_layout

\begin_layout Standard
According to another resource 
\begin_inset CommandInset citation
LatexCommand cite
key "Venter2007"

\end_inset

, it is actually possible to relax the exponential family assumption considerabl
y, though the math becomes potentially more difficult (i.e., it requires brain
 time).
 My exploration of generalized linear models is motivated partly by another
 paper I read on combining the cox model with MARS to perform survival analysis
\begin_inset CommandInset citation
LatexCommand cite
key "LeBlanc1999"

\end_inset

.
 In this paper, the authors used a reweighting method similar to iteratively
 reweighted least squares to adapt a weighted MARS implementation to maximize
 the Cox partial likelihood function (instead of a normal squared error
 loss).
 It wasn't clear how successful their method really was, but I think its
 deficiencies were due to the fact that they did not perform iterative reweighti
ng, but rather only a single weight computation.
\end_layout

\begin_layout Standard
All this reading has given me an idea.
 Generalized linear models relax parts 1 and 3 of the specification of general
 linear models from 
\begin_inset CommandInset citation
LatexCommand cite
key "Mccullagh1989"

\end_inset

.
 Is it possible also to relax part 2? For example, could I replace the linear
 predictor with a MARS predictor? Could I use any regressor capable of handling
 weighted samples? That's what I'd like to investigate here.
 Firstly, notation.
 Let 
\begin_inset Formula $\mathbf{X}$
\end_inset

 be the matrix of independent variables (not necessarily statistically independe
nt, obviously) with columns 
\begin_inset Formula $\mathbf{x_{j}}$
\end_inset

 and entries 
\begin_inset Formula $x_{ij}$
\end_inset

.
 Let 
\begin_inset Formula $\mathbf{y}$
\end_inset

 be the observed response.
 Let 
\begin_inset Formula $g\left(\cdot\right)$
\end_inset

 be the link function such that 
\begin_inset Formula $\mathbf{\eta}=g\left(\mathbf{\mu}\right)$
\end_inset

 and overloaded such that 
\begin_inset Formula $\eta_{i}=g\left(\mu_{i}\right)$
\end_inset

, with 
\begin_inset Formula $\mathbf{\eta}$
\end_inset

 the regression predictor (not necessarily linear any longer) and 
\begin_inset Formula $\mathbf{\mu}$
\end_inset

 the expected value of the response distribution such that 
\begin_inset Formula $\mathbf{Y}\sim f_{Y}\left(\mathbf{\mu}\right)$
\end_inset

, as always, with 
\begin_inset Formula $f\left(\cdot\right)$
\end_inset

 the response distribution, which possibly has some repressed additional
 parameters.
 Introduce the notation 
\begin_inset Formula $h\left(\cdot;\cdot\right)$
\end_inset

 for the regression function, with parameters/structure 
\begin_inset Formula $\mathbf{\beta}$
\end_inset

 so that 
\begin_inset Formula $\mathbf{\eta}=h\left(\mathbf{X};\mathbf{\beta}\right)$
\end_inset

.
 
\end_layout

\begin_layout Standard
Let's dive in.
 A usual way to fit a generalized linear model is as follows:
\end_layout

\begin_layout Enumerate
Let 
\begin_inset Formula $\mathbf{\hat{\mu}_{0}}=\mathbf{y}$
\end_inset

 and 
\begin_inset Formula $k=0$
\end_inset


\end_layout

\begin_layout Enumerate
Until convergence, do:
\end_layout

\begin_deeper
\begin_layout Enumerate
\begin_inset Formula $\mathbf{\hat{\eta}_{k}}=g\left(\mathbf{\hat{\mu}_{k}}\right)$
\end_inset


\end_layout

\begin_layout Enumerate
\begin_inset Formula $\mathbf{z_{k}}=\mathbf{\hat{\eta}_{k}}+\left(\mathbf{y}-\mathbf{\hat{\mu}_{k}}\right)\dot{g}\left(\mathbf{\hat{\mu}_{k}}\right)$
\end_inset


\end_layout

\begin_layout Enumerate
\begin_inset Formula $\mathbf{w_{k}^{-1}}=\left(\dot{g}\left(\mathbf{\hat{\mu}_{k}}\right)\right)^{2}V\left(\mathbf{\hat{\mu}_{k}}\right)$
\end_inset


\end_layout

\begin_layout Enumerate
Regress 
\begin_inset Formula $\mathbf{z_{k}}$
\end_inset

 on 
\begin_inset Formula $\mathbf{X}$
\end_inset

 with weights 
\begin_inset Formula $\mathbf{w_{k}}$
\end_inset

 to get 
\begin_inset Formula $\mathbf{\hat{\eta}_{k+1}}$
\end_inset


\end_layout

\begin_layout Enumerate
Set 
\begin_inset Formula $k=k+1$
\end_inset


\end_layout

\end_deeper
\begin_layout Standard
This is iteratively reweighted least squares (IRLS).
 In the case of generalized linear models, the regression step is linear
 regression and 
\begin_inset Formula $V\left(\cdot\right)$
\end_inset

 is the variance function for the response distribution (which is from an
 exponential family).
 Why does this work? Well, IRLS is just a special case of Fisher's scoring
 method.
 More specifically, it is shown in section 2.5.1 of 
\begin_inset CommandInset citation
LatexCommand cite
key "Mccullagh1989"

\end_inset

 that the likelihood equations of any response distribution from an exponential
 family are 
\begin_inset Formula 
\begin{equation}
\sum_{i}w_{i}\left(y_{i}-\mu_{i}\right)\dot{g}\left(\hat{\mu}_{ki}\right)x_{ij}=0,\forall j
\end{equation}

\end_inset

By likelihood equations, they mean the equations 
\begin_inset Formula $\frac{\partial l}{\partial\beta_{j}}=0$
\end_inset

, where 
\begin_inset Formula $l$
\end_inset

 is the log likelihood function.
 Let 
\begin_inset Formula $\mathbf{u}=\frac{\partial l}{\partial\mathbf{\beta}}$
\end_inset

.
 They further show that the negative expected value of the Hessian matrix,
 
\begin_inset Formula $\mathbf{A}=-E\left(\frac{\partial^{2}l}{\partial\beta_{r}\partial\beta_{s}}\right)$
\end_inset

, is given by 
\begin_inset Formula 
\begin{eqnarray}
A_{rs} & = & -E\left(\frac{\partial u_{r}}{\partial\beta_{s}}\right)\\
 & = & -E\left[\sum_{i}w_{i}x_{ir}x_{is}\right]
\end{eqnarray}

\end_inset

They then show that the Newton's method step equation 
\begin_inset Formula 
\begin{equation}
\mathbf{A}\delta\mathbf{b}=\mathbf{u}
\end{equation}

\end_inset

defining update 
\begin_inset Formula $\mathbf{b^{\ast}}=\mathbf{b}+\delta\mathbf{b}$
\end_inset

 is equivalent to
\begin_inset Formula 
\begin{equation}
\left(\mathbf{A}\mathbf{b^{\ast}}\right)_{r}=\sum_{i}\mathbf{W}\mathbf{x}_{r}\left\{ \mathbf{\eta}+\left(\mathbf{y}-\mathbf{\mu}\right)\dot{g}\left(\mathbf{\mu}\right)\right\} 
\end{equation}

\end_inset


\end_layout

\begin_layout Section*
What if my model is a function?
\end_layout

\begin_layout Standard
I mean, obviously my model is a function.
 But usually you think of a GLM as being a set of parameters, 
\begin_inset Formula $\mathbf{\beta}$
\end_inset

.
 But I'm saying there are no parameters, or at least that there is not a
 fixed dimensional vector of parameters that completely defines my model
 and over which I will optimize.
 Look, my model is just a function, call it 
\begin_inset Formula $\phi$
\end_inset

, and suppose that there is some set, 
\begin_inset Formula $\psi$
\end_inset

, of functions that defines the space of possible models.
 MARS is a good example.
 MARS finds a function that approximates some other function from which
 there are samples available.
 That's what every regression method does.
 MARS also uses a GCV criterion to prevent overfitting the sample.
 Basically, the GCV is a functional and MARS finds the function 
\begin_inset Formula $\phi\in\psi$
\end_inset

 that minimizes it (or at least makes it pretty small).
 Every regression method works this way, from linear regression to random
 forests.
 
\end_layout

\begin_layout Standard
I want my MARS, though, instead of minimizing the 
\begin_inset Formula $\text{GCV}\left(y,\hat{y}\right)$
\end_inset

, to maximize some log likelihood 
\begin_inset Formula $l\left(y,\hat{y}\right)$
\end_inset

.
 Is there a way I can turn the maximum likelihood problem into a minimum
 GCV problem? Suppose it is possible to write the log likelihood as a function
 of 
\begin_inset Formula $\nu\left(y\right)-\hat{y}$
\end_inset

, so 
\begin_inset Formula $l\left(\nu\left(y\right),\hat{y}\right)=l\left(\nu\left(y\right)-\hat{y}\right)$
\end_inset

, where 
\begin_inset Formula $\nu$
\end_inset

 is some known function.
 I propose the following procedure.
 Let 
\begin_inset Formula $z=\nu\left(y\right)-\hat{y}$
\end_inset

.
 Let 
\begin_inset Formula $R$
\end_inset

 be my regression method, so that 
\begin_inset Formula $\phi=R\left(X,y,w\right)$
\end_inset

 runs the method with predictors 
\begin_inset Formula $X$
\end_inset

, response 
\begin_inset Formula $y$
\end_inset

, and sample weights 
\begin_inset Formula $w$
\end_inset

, and produces the resulting model function 
\begin_inset Formula $\phi$
\end_inset

 (which in the case of MARS optimizes the GCV with sample weights 
\begin_inset Formula $w$
\end_inset

).
 Do the following:
\end_layout

\begin_layout Enumerate
Let 
\begin_inset Formula $w_{0}=\mathbf{1}$
\end_inset


\end_layout

\begin_layout Enumerate
Let 
\begin_inset Formula $\phi_{0}=R\left(X,\nu\left(y\right),w_{0}\right)$
\end_inset


\end_layout

\begin_layout Enumerate
Let 
\begin_inset Formula $i=0$
\end_inset


\end_layout

\begin_layout Enumerate
Until converged, do:
\end_layout

\begin_deeper
\begin_layout Enumerate
Let 
\begin_inset Formula $\hat{y}_{i}=\phi_{i}\left(X\right)$
\end_inset


\end_layout

\begin_layout Enumerate
Compute new weights 
\begin_inset Formula $w_{i+1}=\delta\left(\left.\frac{dl}{dz}\right|_{\hat{y}_{i}}\right)$
\end_inset


\end_layout

\begin_layout Enumerate
Let 
\begin_inset Formula $\phi_{i+1}=R\left(X,\nu\left(y\right),w_{i+1}\right)$
\end_inset


\end_layout

\begin_layout Enumerate
\begin_inset Formula $i\equiv i+1$
\end_inset


\end_layout

\end_deeper
\begin_layout Standard
where 
\begin_inset Formula $\delta$
\end_inset

 is some monotonic function I've yet to work out.
 Maybe it's just the identity function.
 The heuristic argument is this: At each step, the algorithm figures out
 which sample points would provide the most improvement in likelihood if
 they moved closer to 
\begin_inset Formula $\nu\left(y\right)$
\end_inset

 and weights them more highly, so that at the next iteration the likelihood
 should be improved.
 It may be possible to do this without iterating: that is, to find the optimal
 weights right away from the start.
 But that would require more math and that math would be specific to the
 particular 
\begin_inset Formula $R$
\end_inset

.
 This method is nice because it could work for any 
\begin_inset Formula $R$
\end_inset

, although it may be that different 
\begin_inset Formula $R$
\end_inset

s will require different 
\begin_inset Formula $\delta$
\end_inset

s.
\end_layout

\begin_layout Section*
Complete separation
\end_layout

\begin_layout Standard
The algorithm in the previous section is actually just a more general version
 of the IRLS algorithm.
 I did some more research RE: can I use IRLS with 
\begin_inset Quotes eld
\end_inset

base regressors
\begin_inset Quotes erd
\end_inset

 other than linear regression?.
 It turns out that is pretty much exactly how generalized additive models
 (GAM) work.
 So, I'm going to just try it and see how it works.
 I've implemented a basic IRLS regressor using a scikit-learn style interface,
 and I can plug in any sklearn-conformant regressor (such as pyearth models).
 I'm testing it out first with plain old GLM models, and am encountering
 difficulties with convergence for logistic regression.
 The difficulty results from cases where 
\begin_inset Formula $\mu_{i}=0$
\end_inset

 or 
\begin_inset Formula $\mu_{i}=1$
\end_inset

 for some 
\begin_inset Formula $i$
\end_inset

 at some step of the algorithm.
 I think this happens because of 
\begin_inset Quotes eld
\end_inset

complete separation
\begin_inset Quotes erd
\end_inset

 or 
\begin_inset Quotes eld
\end_inset

quasi-complete separation
\begin_inset Quotes erd
\end_inset

 in the training data 
\begin_inset CommandInset citation
LatexCommand cite
key "Altman"

\end_inset

.
 It's not clear to me how the software should handle these situations.
 
\end_layout

\begin_layout Standard
Update: I'm solving the complete separation problem by checking the domain
 of the link function against 
\begin_inset Formula $\mu$
\end_inset

 at each step.
 If the domain is violated, I issue a warning and break out of the algorithm.
\end_layout

\begin_layout Section*
Survival Analysis
\end_layout

\begin_layout Standard
Of course, the goal of all this is nonparametric survival analysis, preferably
 with scikit-learn but at least with MARS.
 Let's start with proportional hazards.
 The hazard function is 
\begin_inset Formula 
\begin{eqnarray}
h\left(\mathbf{x},t\right) & = & \lambda_{0}\left(t\right)e^{\phi\left(\mathbf{x}\right)}\\
 & = & \lambda_{0}\left(t\right)\mathbf{\mu}
\end{eqnarray}

\end_inset

where 
\begin_inset Formula $\phi$
\end_inset

 is the regression model and I have implicitly defined my link function
 as 
\begin_inset Formula $\mathbf{\mu}=e^{\mathbf{\eta}}=e^{\phi\left(\mathbf{x}\right)}$
\end_inset

.
 Let 
\begin_inset Formula $\mathbf{y}$
\end_inset

 be the survival times and 
\begin_inset Formula $\mathbf{c}$
\end_inset

 be the censoring status, where 
\begin_inset Formula $c_{i}=1$
\end_inset

 if unit 
\begin_inset Formula $i$
\end_inset

 experienced an event and 
\begin_inset Formula $0$
\end_inset

 otherwise.
 The partial likelihood function is 
\begin_inset Formula 
\begin{eqnarray}
l\left(\mathbf{\mu}\right) & = & \sum_{i:c_{i}=1}\left(\log\left(\mu_{i}\right)-\log\left(\sum_{j:y_{j}>y_{i}}\mu_{j}\right)\right)\\
 & = & \sum_{i=1}^{n}\left[c_{i}\log\left(\mu_{i}\right)-c_{i}\log\left(\sum_{j=1}^{n}\mu_{j}I_{y_{j}>y_{i}}\right)\right]\\
 & = & \left(\sum_{i=1}^{n}c_{i}\log\left(\mu_{i}\right)\right)-\left(\sum_{i=1}^{n}c_{i}\log\left(\sum_{j=1}^{n}\mu_{j}I_{y_{j}>y_{i}}\right)\right)
\end{eqnarray}

\end_inset

The important derivatives are
\begin_inset Formula 
\begin{equation}
\frac{\partial l}{\partial\mu_{k}}=\frac{c_{k}}{\mu_{k}}-\sum_{i=1}^{n}\frac{c_{i}I_{y_{k}>y_{i}}}{\sum_{j=1}^{n}\mu_{j}I_{y_{j}>y_{i}}}
\end{equation}

\end_inset

and 
\begin_inset Formula 
\begin{equation}
\frac{\partial^{2}l}{\partial\mu_{k}^{2}}=-\frac{c_{k}}{\mu_{k}^{2}}+\sum_{i=1}^{n}\frac{c_{i}I_{y_{k}>y_{i}}}{\left(\sum_{j=1}^{n}\mu_{j}I_{y_{j}>y_{i}}\right)^{2}}
\end{equation}

\end_inset

There are also off-diagonal elements of the Hessian, so actually 
\begin_inset Formula 
\begin{equation}
\frac{d^{2}l}{d\mu_{k_{1}}d\mu_{k_{2}}}=-\frac{c_{k_{1}}}{\mu_{k_{1}}^{2}}I_{k_{1}=k_{2}}+\sum_{i=1}^{n}\frac{c_{i}I_{y_{k_{1}}>y_{i}}I_{y_{k_{2}}>y_{i}}}{\left(\sum_{j=1}^{n}\mu_{j}I_{y_{j}>y_{i}}\right)^{2}}
\end{equation}

\end_inset

Note that ties are not treated specially.
 Therefore this likelihood and its derivatives correspond to Breslow's method.
 Efron's method might be better, but let's keep it simple for now.
\end_layout

\begin_layout Section*
What if I drop the proportional hazards assumption?
\end_layout

\begin_layout Standard
Instead, let 
\begin_inset Formula 
\begin{eqnarray}
h\left(\mathbf{x},t\right) & = & e^{\phi\left(\mathbf{x},t\right)}\\
 & = & \mathbf{\mu}
\end{eqnarray}

\end_inset

I need to figure out the likelihood function.
 Again, let 
\begin_inset Formula $\mathbf{y}$
\end_inset

 be the times and 
\begin_inset Formula $\mathbf{c}$
\end_inset

 the censoring type, with 
\begin_inset Formula $c_{i}=1$
\end_inset

 indicating a failure / death / event occurred for statistical unit 
\begin_inset Formula $i$
\end_inset

.
 The pdf is 
\begin_inset Formula 
\begin{equation}
f\left(\mathbf{x},t\right)=S\left(\mathbf{x},t\right)h\left(\mathbf{x},t\right)
\end{equation}

\end_inset

and the cdf is 
\begin_inset Formula 
\begin{equation}
F\left(\mathbf{x},t\right)=1-S\left(\mathbf{x},t\right)
\end{equation}

\end_inset

The survival function is 
\begin_inset Formula 
\begin{equation}
S\left(\mathbf{x},t\right)=e^{-H\left(\mathbf{x},t\right)}
\end{equation}

\end_inset

with 
\begin_inset Formula $H\left(\mathbf{x},t\right)=\int_{0}^{t}h\left(\mathbf{x},\tau\right)d\tau$
\end_inset

.
 Then the likelihood function is 
\begin_inset Formula 
\begin{eqnarray}
L & = & \prod_{i=1}^{n}f\left(\mathbf{x},y_{i}\right)^{c_{i}}\left(S\left(\mathbf{x},y_{i}\right)\right)^{1-c_{i}}
\end{eqnarray}

\end_inset

Then the log likelihood is 
\begin_inset Formula 
\begin{equation}
l=\sum_{i=1}^{n}c_{i}\phi\left(\mathbf{x},y_{i}\right)-\int_{0}^{y_{i}}e^{\phi\left(\mathbf{x},\tau\right)}d\tau
\end{equation}

\end_inset

which depends not only on 
\begin_inset Formula $\mu_{i}=e^{\phi\left(\mathbf{x},y_{i}\right)}$
\end_inset

 but also on 
\begin_inset Formula $\int_{0}^{y_{i}}e^{\phi\left(\mathbf{x},\tau\right)}d\tau$
\end_inset

.
 That's a hard problem.
 Maybe there is a variational method that could be used, but it's hard to
 see how this problem could be solved with IRLS, since the likelihood depends
 on 
\begin_inset Formula $\phi$
\end_inset

 and not just on its values at the times 
\begin_inset Formula $\mathbf{y}$
\end_inset

.
 It's easy to take its 
\begin_inset Formula $\mathbf{y}$
\end_inset

 derivatives.
\begin_inset Formula 
\begin{equation}
\frac{\partial l}{\partial y_{k}}=c_{k}\frac{\partial\phi}{\partial y_{k}}\left(\mathbf{x},y_{k}\right)-e^{\phi\left(\mathbf{x},y_{k}\right)}
\end{equation}

\end_inset

Furthermore, 
\begin_inset Formula $\frac{\partial y_{k}}{\partial\mu_{k}}=\frac{\partial}{\partial\mu_{k}}\log\left(\mu_{k}\right)=\frac{1}{\mu_{k}}$
\end_inset

.
 Then the derivative would be
\begin_inset Formula 
\begin{eqnarray}
\frac{\partial l}{\partial\mu_{k}} & = & e^{-\phi\left(\mathbf{x},y_{k}\right)}\left(c_{k}\frac{\partial\phi}{\partial y_{k}}\left(\mathbf{x},y_{k}\right)-e^{\phi\left(\mathbf{x},y_{k}\right)}\right)\\
 & = & c_{k}e^{-\phi\left(\mathbf{x},y_{k}\right)}\frac{\partial\phi}{\partial y_{k}}\left(\mathbf{x},y_{k}\right)-1\\
 & = & c_{k}e^{-\phi\left(\mathbf{x},y_{k}\right)}\frac{\partial\phi}{\partial y_{k}}\left(\mathbf{x},y_{k}\right)-1
\end{eqnarray}

\end_inset

or is that just a frightful abuse of notation? I think the units check out.
 If it works, I have traded an integral of 
\begin_inset Formula $e^{\phi\left(\mathbf{x},t\right)}$
\end_inset

 for a derivative of the same, which is a substantial improvement, particularly
 if 
\begin_inset Formula $\phi$
\end_inset

 is a MARS model.
 The next derivative would be 
\begin_inset Formula 
\begin{equation}
\frac{\partial^{2}l}{\partial\mu_{k}\partial y_{k}}=c_{k}e^{-\phi\left(\mathbf{x},y_{k}\right)}\left(\frac{\partial^{2}\phi}{\partial y_{k}^{2}}\left(\mathbf{x},y_{k}\right)-\left(\frac{\partial\phi}{\partial y_{k}}\left(\mathbf{x},y_{k}\right)\right)^{2}\right)
\end{equation}

\end_inset

and applying the chain rule gives 
\begin_inset Formula 
\begin{equation}
\frac{\partial^{2}l}{\partial\mu_{k}\partial\mu_{k}}=c_{k}e^{-2\phi\left(\mathbf{x},y_{k}\right)}\left(\frac{\partial^{2}\phi}{\partial y_{k}^{2}}\left(\mathbf{x},y_{k}\right)-\left(\frac{\partial\phi}{\partial y_{k}}\left(\mathbf{x},y_{k}\right)\right)^{2}\right)
\end{equation}

\end_inset

and for off-diagonal elements, 
\begin_inset Formula 
\begin{equation}
\frac{\partial^{2}l}{\partial\mu_{k_{1}}\partial y_{k_{2}}}=0,k_{1}\ne k_{2}
\end{equation}

\end_inset

Now there is a second derivative of 
\begin_inset Formula $\phi$
\end_inset

, which can still be approximated fairly easily in general or calculated
 exactly for a MARS model.
 However, my current MARS implementation does not support second derivatives
 and even smoothed MARS does not have continuous second derivatives.
 Some types of models won't even have continuous first derivatives, or may
 not be continuous functions themselved.
\end_layout

\begin_layout Subsection*
Conclusion
\end_layout

\begin_layout Standard
This general case cannot be solved with IRLS.
 Could it be solved with GIRLS
\begin_inset CommandInset citation
LatexCommand cite
key "Bissantz2009"

\end_inset

? With sieves
\begin_inset CommandInset citation
LatexCommand cite
key "Geman1982"

\end_inset

? With a variational approach?
\end_layout

\begin_layout Subsection*
Well, actually...
\end_layout

\begin_layout Standard
I guess what I'm looking for is 
\begin_inset Formula $\frac{\partial l}{\partial\mu_{k}}$
\end_inset

 where 
\begin_inset Formula $\mu_{k}=e^{\phi\left(\mathbf{x},y_{k}\right)}$
\end_inset

.
 So let 
\begin_inset Formula 
\begin{equation}
\psi=\int_{0}^{y_{i}}e^{\phi\left(\mathbf{x},\tau\right)}d\tau
\end{equation}

\end_inset

The derivative is approximately 
\begin_inset Formula $\frac{\Delta\psi}{\Delta\mu_{k}}$
\end_inset

 and the change in 
\begin_inset Formula $\mu_{k}$
\end_inset

 is because of a change in 
\begin_inset Formula $\phi\left(\mathbf{x},y_{k}\right)$
\end_inset

.
 If I assume that change basically happens 
\emph on
at
\emph default
 
\begin_inset Formula $y_{k}$
\end_inset

, then could I approximate the derivative as 
\begin_inset Formula $\frac{\partial\psi}{\mu_{k}}\approx1$
\end_inset

? Maybe I should run some numerical experiments.
 Let's run with that idea, though.
 Even if it's right, is 
\begin_inset Formula $\mu_{k}$
\end_inset

 even the thing to optimize over for the 
\begin_inset Formula $c_{k}=0$
\end_inset

 cases? Maybe there are two sets of parameters, 
\begin_inset Formula $\mu_{k}$
\end_inset

 and its integral! That makes some degree of sense, since there are now
 twice as many outcome variables.
 That is, there is 
\begin_inset Formula $\mathbf{c}$
\end_inset

 in addition to 
\begin_inset Formula $\mathbf{y}$
\end_inset

.
 Okay, so let's treat 
\begin_inset Formula $\mu_{k}$
\end_inset

 and 
\begin_inset Formula $\nu_{k}=\int_{0}^{y_{k}}e^{\phi\left(\mathbf{x},\tau\right)}d\tau$
\end_inset

 as independent and find the derivatives with respect to each.
 
\begin_inset Formula 
\begin{equation}
l\left(\mathbf{\mu},\mathbf{\nu}\right)=\sum_{i=1}^{n}c_{i}\log\left(\mu_{k}\right)-\nu_{k}
\end{equation}

\end_inset

Then 
\begin_inset Formula 
\begin{eqnarray}
\frac{\partial l}{\partial\mu_{k}} & = & \frac{c_{i}}{\mu_{k}}\\
\frac{\partial^{2}l}{\partial\mu_{k_{1}}\partial\mu_{k_{2}}} & = & -\frac{c_{i}}{\mu_{k}^{2}}I_{k_{1}=k_{2}}\\
\frac{\partial l}{\partial\nu_{k}} & = & -1\\
\frac{\partial^{2}l}{\partial\nu_{k_{1}}\partial\nu_{k_{2}}} & = & 0\\
\frac{\partial^{2}l}{\partial\nu_{k_{1}}\partial\mu_{k_{2}}} & = & 0\\
\frac{\partial^{2}l}{\partial\mu_{k_{1}}\partial\nu_{k_{2}}} & = & 0
\end{eqnarray}

\end_inset


\end_layout

\begin_layout Section*
IDK
\end_layout

\begin_layout Standard
I'm not really convinced by those derivatives in the non-proportional hazards
 models, nor do I fully understand when and why IRLS works.
 Let's try to work it out in general.
 Let 
\begin_inset Formula $l$
\end_inset

 be the loss and 
\begin_inset Formula $\mu$
\end_inset

 the link, with 
\begin_inset Formula $\eta$
\end_inset

 the linear predictor.
 The loss function is a function of the data and of 
\begin_inset Formula $\mu$
\end_inset

, and I want the root of its first derivative.
\begin_inset Formula 
\begin{equation}
\frac{\partial l}{\partial\beta}=0
\end{equation}

\end_inset

where 
\begin_inset Formula $ $
\end_inset


\begin_inset Formula $\beta$
\end_inset

 is the set of model parameters and 
\begin_inset Formula $\frac{\partial l}{\partial\beta}$
\end_inset

 is a function of the data.
 The usual Newton step is derived by solving 
\begin_inset Formula 
\begin{equation}
\frac{\partial l}{\partial\beta}+\frac{\partial^{2}l}{\partial\beta^{2}}\Delta\beta^{\left(t\right)}=0
\end{equation}

\end_inset

and the step is 
\begin_inset Formula 
\begin{equation}
\beta^{\left(t+1\right)}=\beta^{\left(t\right)}+\Delta\beta^{\left(t\right)}.
\end{equation}

\end_inset

The Hessian can be expanded using the chain rule
\begin_inset Formula 
\begin{equation}
\frac{\partial^{2}l}{\partial\beta^{2}}=\frac{\partial^{2}l}{\partial\mu^{2}}\left(\frac{\partial\mu}{\partial\eta}\right)^{2}\left(\frac{\partial\eta}{\partial\beta}\right)^{2}
\end{equation}

\end_inset

and the score function is 
\begin_inset Formula 
\begin{equation}
\frac{\partial l}{\partial\beta}=\frac{\partial l}{\partial\mu}\frac{\partial\mu}{\partial\eta}\frac{\partial\eta}{\partial\beta}
\end{equation}

\end_inset

In Fisher scoring you instead use the expected value of the Hessian, 
\begin_inset Formula 
\begin{equation}
A=-E\left(\frac{\partial^{2}l}{\partial\beta^{2}}\right)
\end{equation}

\end_inset

so that 
\begin_inset Formula 
\begin{equation}
\frac{\partial l}{\partial\beta}-A\Delta\beta^{\left(t\right)}=0
\end{equation}

\end_inset

but whatever.
 If you only care about exponential families then I guess that makes things
 simpler somehow, but forget it for now.
 Anyway, we have
\begin_inset Formula 
\begin{eqnarray}
\beta^{\left(t+1\right)} & = & \beta^{\left(t\right)}+\Delta\beta^{\left(t\right)}\\
 & =
\end{eqnarray}

\end_inset

 
\begin_inset Formula 
\begin{eqnarray}
\Delta\beta^{\left(t\right)} & = & -\left(\frac{\partial^{2}l}{\partial\beta^{2}}\right)^{-1}\frac{\partial l}{\partial\beta}\\
 & = & -\left(\frac{\partial^{2}l}{\partial\mu^{2}}\left(\frac{\partial\mu}{\partial\eta}\right)^{2}\left(\frac{\partial\eta}{\partial\beta}\right)^{2}\right)^{-1}\frac{\partial l}{\partial\mu}\frac{\partial\mu}{\partial\eta}\frac{\partial\eta}{\partial\beta}\\
\left(\frac{\partial\eta}{\partial\beta}\right)^{2}\Delta\beta^{\left(t\right)} & = & \left(\frac{\partial^{2}l}{\partial\mu^{2}}\left(\frac{\partial\mu}{\partial\eta}\right)^{2}\right)^{-1}\frac{\partial l}{\partial\mu}\frac{\partial\mu}{\partial\eta}\frac{\partial\eta}{\partial\beta}\\
\frac{\partial^{2}l}{\partial\mu^{2}}\left(\frac{\partial\mu}{\partial\eta}\right)^{2}\left(\frac{\partial\eta}{\partial\beta}\right)^{2}\left(\beta^{\left(t+1\right)}-\beta^{\left(t\right)}\right) & = & \frac{\partial l}{\partial\mu}\frac{\partial\mu}{\partial\eta}\frac{\partial\eta}{\partial\beta}\\
\left(\frac{\partial\eta}{\partial\beta}\right)^{2}\beta^{\left(t+1\right)} & = & \left(\frac{\partial^{2}l}{\partial\mu^{2}}\left(\frac{\partial\mu}{\partial\eta}\right)^{2}\right)^{-1}\frac{\partial l}{\partial\mu}\frac{\partial\mu}{\partial\eta}\frac{\partial\eta}{\partial\beta}+\left(\frac{\partial\eta}{\partial\beta}\right)^{2}\beta^{\left(t\right)}
\end{eqnarray}

\end_inset


\end_layout

\begin_layout Section*
Ignore that last section
\end_layout

\begin_layout Standard
Herein I can finally explain IRLS.
 I really think I get it now! The following is based closely on the explanation
 in 
\begin_inset CommandInset citation
LatexCommand cite
key "Green2011"

\end_inset

.
 Okay, the goal is to solve 
\begin_inset Formula 
\begin{equation}
\frac{\partial l}{\partial\beta}=0
\end{equation}

\end_inset

and note that 
\begin_inset Formula $\frac{\partial l}{\partial\beta}=D^{T}u$
\end_inset

 where 
\begin_inset Formula $u=\frac{\partial l}{\partial\eta}$
\end_inset

 and 
\begin_inset Formula $D=\frac{\partial\eta}{\partial\beta}$
\end_inset

.
 So we're solving 
\begin_inset Formula 
\begin{equation}
D^{T}u=0
\end{equation}

\end_inset

for 
\begin_inset Formula $\beta$
\end_inset

.
 We use Newton-Raphson, so the step is 
\begin_inset Formula 
\begin{eqnarray}
\frac{\partial^{2}l}{\partial\beta^{2}}\beta^{\left(t+1\right)} & = & \frac{\partial^{2}l}{\partial\beta^{2}}\beta^{\left(t\right)}-\frac{\partial l}{\partial\beta}\\
 & = & \frac{\partial^{2}l}{\partial\beta^{2}}\beta^{\left(t\right)}-D^{T}u\\
-\frac{\partial^{2}l}{\partial\beta^{2}}\left(\beta^{\left(t+1\right)}-\beta^{\left(t\right)}\right) & = & D^{T}u
\end{eqnarray}

\end_inset

That is, you obtain 
\begin_inset Formula $\beta^{\left(t+1\right)}$
\end_inset

 by solving the above linear system.
 That's just basic Newton-Raphson.
 The trick to turning plain old Newton-Raphson into IRLS is to employ a
 particular approximation, called Fisher scoring.
 In Fisher scoring, you approximate the negative Hessian 
\begin_inset Formula $-\frac{\partial^{2}l}{\partial\beta^{2}}$
\end_inset

 by the Fisher information matrix, which gives 
\begin_inset Formula 
\begin{eqnarray}
-\frac{\partial^{2}l}{\partial\beta^{2}} & \approx & E\left(-\frac{\partial^{2}l}{\partial\beta^{2}}\right)\label{eq:approximation}\\
 & = & \left(\frac{\partial\eta}{\partial\beta}\right)^{T}E\left(\frac{\partial l}{\partial\eta}\left(\frac{\partial l}{\partial\eta}\right)^{T}\right)\frac{\partial\eta}{\partial\beta}\label{eq:after_approximation}\\
 & = & D^{T}E\left(\frac{\partial l}{\partial\eta}\left(\frac{\partial l}{\partial\eta}\right)^{T}\right)D
\end{eqnarray}

\end_inset

where I skipped a few steps between the 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:approximation"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:after_approximation"

\end_inset

 that are shown more thoroughly in 
\begin_inset CommandInset citation
LatexCommand cite
key "Green2011"

\end_inset

.
 Substituting this approximation into the equation for the Newton-Raphson
 step gives 
\begin_inset Formula 
\begin{equation}
D^{T}E\left(\frac{\partial l}{\partial\eta}\left(\frac{\partial l}{\partial\eta}\right)^{T}\right)D\left(\beta^{\left(t+1\right)}-\beta^{\left(t\right)}\right)=D^{T}u
\end{equation}

\end_inset

If you look at this last system long enough, you see that it is the just
 the normal equations for a weighted linear regression problem with target
 
\begin_inset Formula 
\begin{eqnarray}
z & = & E\left(\frac{\partial l}{\partial\eta}\left(\frac{\partial l}{\partial\eta}\right)^{T}\right)^{-1}u+D\beta,
\end{eqnarray}

\end_inset

onto 
\begin_inset Formula $D$
\end_inset

 with weight matrix 
\begin_inset Formula 
\begin{equation}
W=E\left(\frac{\partial l}{\partial\eta}\left(\frac{\partial l}{\partial\eta}\right)^{T}\right)
\end{equation}

\end_inset


\end_layout

\begin_layout Subsection*
Optimizing machine
\end_layout

\begin_layout Standard
How can the above discussion be generalized to include models 
\begin_inset Formula $\eta=h\left(x\right)$
\end_inset

 where 
\begin_inset Formula $h$
\end_inset

 is 
\emph on
not
\emph default
 parameterized by some fixed dimensional vector 
\begin_inset Formula $\beta$
\end_inset

.
 Let's just think about why IRLS works.
 It is based on the idea of approximating the loss function by the first
 few terms of its Taylor series,
\begin_inset Formula 
\begin{eqnarray}
l\left(\mu_{0}+\Delta\mu,y\right) & \approx & l\left(\mu_{0},y\right)+\frac{\partial l}{\partial\mu}\left(\mu_{0},y\right)\Delta\mu+\frac{1}{2}\Delta\mu^{T}\frac{\partial^{2}l}{\partial\mu^{2}}\left(\mu_{0},y\right)\Delta\mu\label{eq:taylor_loss}\\
 & = & l\left(\mu_{0},y\right)+\frac{\partial l}{\partial\mu}\left(\mu_{0},y\right)\left(\mu^{\left(t+1\right)}-\mu^{\left(t\right)}\right)+\frac{1}{2}\left(\mu^{\left(t+1\right)}-\mu^{\left(t\right)}\right)^{T}\frac{\partial^{2}l}{\partial\mu^{2}}\left(\mu_{0},y\right)\left(\mu^{\left(t+1\right)}-\mu^{\left(t\right)}\right)
\end{eqnarray}

\end_inset

and then optimizing this simpler function for 
\begin_inset Formula $\Delta\mu$
\end_inset

.
 That is, set its derivative to 0,
\begin_inset Formula 
\begin{eqnarray}
\frac{\partial l}{\partial\mu}\left(\mu_{0},y\right)+\frac{\partial^{2}l}{\partial\mu^{2}}\left(\mu_{0},y\right)\Delta\mu & = & 0\label{eq:unconstrained_case}\\
-\frac{\partial^{2}l}{\partial\mu^{2}}\left(\mu_{0},y\right)\Delta\mu & = & \frac{\partial l}{\partial\mu}\left(\mu_{0},y\right)
\end{eqnarray}

\end_inset

However, 
\begin_inset Formula $\Delta\mu$
\end_inset

 is not unconstrained.
 In the GLM case, it must be true that 
\begin_inset Formula 
\begin{equation}
g\left(\mu_{0}+\Delta\mu\right)=x\beta
\end{equation}

\end_inset

for some 
\begin_inset Formula $\beta$
\end_inset

, or, slightly more generally, 
\begin_inset Formula 
\begin{equation}
g\left(\mu_{0}+\Delta\mu\right)=h\left(x,\beta\right)
\end{equation}

\end_inset

so that 
\begin_inset Formula 
\begin{equation}
\Delta\mu=g^{-1}\left(h\left(x,\beta\right)\right)-\mu_{0}
\end{equation}

\end_inset

Okay, at each step, optimize 
\begin_inset Formula 
\begin{equation}
l\left(\mu^{\left(t\right)}+\Delta\mu^{\left(t\right)},y\right)\approx l\left(\mu^{\left(t\right)},y\right)+\frac{\partial l}{\partial\mu}\left(\mu^{\left(t\right)},y\right)\Delta\mu^{\left(t\right)}+\frac{1}{2}\frac{\partial^{2}l}{\partial\mu^{2}}\left(\mu_{0},y\right)\left(\Delta\mu^{\left(t\right)}\right)^{2}
\end{equation}

\end_inset

for 
\begin_inset Formula $\Delta\mu^{\left(t\right)}$
\end_inset

 to get 
\begin_inset Formula $\mu^{\left(t+1\right)}=\mu^{\left(t\right)}+\Delta\mu^{\left(t\right)}$
\end_inset

 under the constraint that 
\begin_inset Formula $g\left(\mu_{0}+\Delta\mu\right)=h\left(x\right)$
\end_inset

.
 This is a generalization of the IRLS in the previous section, which I claim
 reduces to that IRLS if 
\begin_inset Formula $h\left(x\right)=h\left(x,\beta\right)$
\end_inset

.
 Firstly, assume the existence of some procedure for solving the following
 problem: find the function 
\begin_inset Formula $h$
\end_inset

 minimizing 
\begin_inset Formula 
\begin{equation}
\left(z-\eta\right)^{T}Q\left(z-\eta\right)
\end{equation}

\end_inset

where 
\begin_inset Formula $h$
\end_inset

 belongs to some space of functions and 
\begin_inset Formula $\eta=h\left(x\right)$
\end_inset

.
 If 
\begin_inset Formula $h\left(x\right)=x\beta$
\end_inset

 then this is just generalized least squares.
 
\end_layout

\begin_layout Standard
\begin_inset Note Comment
status open

\begin_layout Plain Layout
for some 
\begin_inset Formula $\beta$
\end_inset

, or, 
\begin_inset Formula 
\begin{equation}
g\left(\mu\right)=\eta
\end{equation}

\end_inset

so that 
\begin_inset Formula 
\begin{eqnarray}
\frac{\partial\Delta\mu}{\partial\beta} & = & \frac{\partial g^{-1}}{\partial\eta}\frac{\partial\eta}{\partial\beta}\\
 & = & \frac{\partial\eta}{\partial\mu}\frac{\partial\eta}{\partial\beta}
\end{eqnarray}

\end_inset

Okay, so 
\begin_inset Formula 
\begin{equation}
\Delta\mu=g^{-1}\left(h\left(x,\beta\right)\right)-\mu_{0}
\end{equation}

\end_inset

Let's sub that last into 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:taylor_loss"

\end_inset

.
 Then 
\begin_inset Formula 
\begin{equation}
l\left(\mu_{0}+\Delta\mu,y\right)\approx l\left(\mu_{0},y\right)+\frac{\partial l}{\partial\mu}\left(\mu_{0},y\right)\Delta\mu+\frac{1}{2}\frac{\partial^{2}l}{\partial\mu^{2}}\left(\mu_{0},y\right)\Delta\mu^{2}
\end{equation}

\end_inset

Then instead of 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:unconstrained_case"

\end_inset

, the form of the solution becomes 
\begin_inset Formula 
\begin{eqnarray}
\frac{\partial l}{\partial\mu}\left(\mu_{0},y\right)\frac{\partial\Delta\mu}{\partial\beta}+\frac{\partial^{2}l}{\partial\mu^{2}}\left(\mu_{0},y\right)\Delta\mu\frac{\partial\Delta\mu}{\partial\beta} & = & 0\\
\frac{\partial\Delta\mu}{\partial\beta}\left(\frac{\partial l}{\partial\mu}\left(\mu_{0},y\right)+\frac{\partial^{2}l}{\partial\mu^{2}}\left(\mu_{0},y\right)\Delta\mu\right) & = & 0\\
\frac{\partial\mu}{\partial\beta}\left(\frac{\partial l}{\partial\mu}\left(\mu_{0},y\right)+\frac{\partial^{2}l}{\partial^{2}\mu^{2}}\left(\mu_{0},y\right)\left(\mu^{\left(t+1\right)}-\mu^{\left(t\right)}\right)\right) & = & 0\\
\frac{\partial\mu}{\partial\eta}\frac{\partial\eta}{\partial\beta}\left(\frac{\partial l}{\partial\mu}\left(\mu_{0},y\right)+\frac{\partial^{2}l}{\partial\mu^{2}}\left(\mu_{0},y\right)\left(\mu^{\left(t+1\right)}-\mu^{\left(t\right)}\right)\right) & = & 0\\
\frac{\partial\mu}{\partial\eta}D\left(\frac{\partial l}{\partial\mu}\left(\mu_{0},y\right)+\frac{\partial^{2}l}{\partial\mu^{2}}\left(\mu_{0},y\right)\left(\mu^{\left(t+1\right)}-\mu^{\left(t\right)}\right)\right) & = & 0\\
\frac{\partial l}{\partial\beta}+\frac{\partial^{2}l}{\partial\beta\partial\mu}\left(\mu_{0},y\right)\left(\mu^{\left(t+1\right)}-\mu^{\left(t\right)}\right) & = & 0
\end{eqnarray}

\end_inset

Assuming that 
\begin_inset Formula $ $
\end_inset


\begin_inset Formula $h$
\end_inset

 can be inverted, 
\begin_inset Formula 
\begin{eqnarray}
\frac{\partial l}{\partial\beta}+\frac{\partial^{2}l}{\partial\beta\partial\mu}\left(\mu_{0},y\right)\left(\mu^{\left(t+1\right)}-\mu^{\left(t\right)}\right) & = & 0\\
\frac{\partial l}{\partial\beta}+\frac{\partial^{2}l}{\partial\beta\partial\mu}\left(\mu_{0},y\right)\left(g^{-1}\left(h\left(x,\beta^{\left(t+1\right)}\right)\right)-g^{-1}\left(h\left(x,\beta^{\left(t\right)}\right)\right)\right) & = & 0
\end{eqnarray}

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "notes"
options "naturemag"

\end_inset


\end_layout

\end_body
\end_document
